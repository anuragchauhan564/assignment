{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d57440-5c11-4e43-93a0-3fdb346ae9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Difference btween Object Detection and Object Classificatio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315074d3-4b77-4c96-a67e-c1f6cf9d65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a. Explain the difference between object detection and object classification in the\n",
    "    context of computer vision tasks. Provide examples to illustrate each concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b36e1-2e33-48b1-80b6-0aa255491db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Object detection and object classification are both fundamental tasks in computer vision, but\n",
    "      they serve different purposes and involve distinct approaches:\n",
    "    \n",
    "    1. Object Classification:\n",
    "            Object classification involves identifying the category or class of a single object within\n",
    "            an image. It aims to answer the question, \"What is in this image?\" without providing information \n",
    "            about where the object is located within the image. In object classification, the input is an \n",
    "            entire image, and the output is a label or a category assigned to that image.\n",
    "\n",
    "            Example: Suppose you have an image of a cat. Object classification would involve determining\n",
    "            that the image contains a cat, without specifying where the cat is located in the image. The \n",
    "            output would simply be \"cat.\"\n",
    "            \n",
    "    2. Object Detection:\n",
    "            Object detection, on the other hand, involves not only identifying the class of objects within \n",
    "            an image but also determining their locations by drawing bounding boxes around them. It answers \n",
    "            the question, \"What objects are in this image, and where are they located?\" Object detection can \n",
    "            handle scenarios where there are multiple objects of interest in an image.\n",
    "\n",
    "            Example: Consider an image containing various objects like a cat, a dog, and a car. Object detection \n",
    "            would identify each object in the image and draw bounding boxes around them, indicating their precise \n",
    "            locations. The output would include labels such as \"cat,\" \"dog,\" and \"car,\" along with the coordinates \n",
    "            of the bounding boxes around each object.\n",
    "            \n",
    "  object classification focuses solely on determining the class or category of a single object within an image, \n",
    "  while object detection involves identifying multiple objects and their precise locations within the image by \n",
    "  drawing bounding boxes around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da478775-6bf9-4f81-9d1c-bc3f5794135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Scenarios where Object Detection is used:\n",
    "    a. Describe at least three scenarios or real-world applications where object detection\n",
    "        techniques are commonly used. Explain the significance of object detection in these scenarios\n",
    "        and how it benefits the respective applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83a61d-c831-41db-b1da-ea9c92f1a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Object detection techniques find numerous applications across various domains due to their ability \n",
    "       to locate and identify objects within images or video frames. Here are three scenarios where object\n",
    "       detection is commonly used:\n",
    "     \n",
    "        1. Autonomous Vehicles:\n",
    "            In the field of autonomous vehicles, object detection plays a critical role in identifying and \n",
    "            tracking objects such as pedestrians, vehicles, traffic signs, and obstacles in the vehicle's \n",
    "            surroundings. By accurately detecting these objects in real-time, autonomous vehicles can make \n",
    "            informed decisions regarding navigation, speed adjustment, and collision avoidance. Object \n",
    "            detection helps ensure the safety of passengers, pedestrians, and other vehicles on the road \n",
    "            by enabling the vehicle to respond appropriately to its environment.\n",
    "        \n",
    "        2. Surveillance and Security:\n",
    "            Object detection is extensively used in surveillance systems for monitoring public spaces, \n",
    "            buildings, and sensitive areas. It helps in identifying suspicious activities, unauthorized\n",
    "            individuals, or objects of interest within the surveillance footage. By detecting and tracking \n",
    "            objects like intruders, abandoned bags, or vehicles in restricted areas, security personnel can \n",
    "            respond promptly to potential threats and take appropriate actions. Object detection enhances the\n",
    "            effectiveness of surveillance systems by providing real-time alerts and facilitating proactive security measures.\n",
    "            \n",
    "        3. Retail and Inventory Management:\n",
    "            Object detection technology is employed in retail environments for various purposes, including \n",
    "            shelf monitoring, inventory management, and customer behavior analysis. By deploying cameras \n",
    "            equipped with object detection algorithms, retailers can monitor product availability, track \n",
    "            stock levels, and identify out-of-stock or misplaced items on store shelves. Additionally, object \n",
    "            detection enables retailers to analyze customer demographics, preferences, and shopping patterns by \n",
    "            tracking movements and interactions within the store. This information can be leveraged to optimize \n",
    "            product placement, enhance the shopping experience, and improve inventory accuracy, ultimately leading \n",
    "            to increased sales and operational efficiency.\n",
    "            \n",
    "    In these scenarios, object detection techniques provide valuable insights and facilitate decision-making \n",
    "    processes in complex and dynamic environments, thereby enhancing safety, security, and operational efficiency\n",
    "    across various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fb576-935f-4624-b8d4-25db85cafb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Image Data as Structured Data:\n",
    "    a. Discuss whether image data can be considered a structured form of data. Provide reasoning\n",
    "        and examples to support your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b9d21-b0f9-4a32-945f-5bf02a7dbe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Image data can be considered a structured form of data, although it differs significantly from \n",
    "      traditional structured data types like tables or spreadsheets commonly encountered in databases. \n",
    "      Here's why image data can be considered structured:\n",
    "\n",
    "        1. Pixel Arrangement:\n",
    "            While images may seem unstructured at first glance, they actually consist of a grid of pixels, \n",
    "            each having a specific position and color value. This grid structure provides a form of organization \n",
    "            to the data, where each pixel's position corresponds to its location within the image. The arrangement\n",
    "            of pixels follows a definite structure determined by the image's dimensions, such as width and height.\n",
    "            \n",
    "        2. Channel Information:\n",
    "            Images often have multiple channels of data, such as RGB (Red, Green, Blue) or grayscale. Each channel \n",
    "            represents specific information about the image, such as color intensity in the case of RGB images.\n",
    "            The presence of these channels adds another layer of structure to the data, where each channel \n",
    "            contributes to the overall representation of the image.\n",
    "            \n",
    "        3. Metadata:\n",
    "            Image files commonly contain metadata, which provides structured information about the image, such \n",
    "            as the camera settings, creation date, resolution, and format. This metadata adds a level of organization \n",
    "            and context to the image data, making it more structured and informative.\n",
    "        \n",
    "        4.Feature Extraction:\n",
    "            In computer vision tasks, image data can be processed to extract features such as edges, textures, shapes,\n",
    "            and colors. These features provide structured representations of the visual content within the image, \n",
    "            enabling algorithms to analyze and interpret the data effectively.\n",
    "            \n",
    "    While image data possesses inherent structure due to its pixel grid and channel organization, it differs from \n",
    "    traditional structured data in its complexity and multidimensionality. Unlike structured data in tables or \n",
    "    databases, image data requires specialized techniques for processing, analysis, and interpretation. However, \n",
    "    the structured nature of image data facilitates various computer vision tasks, such as object detection, image \n",
    "    classification, and image segmentation, by providing a foundation for algorithms to operate on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428eef01-3102-47b4-bfc7-d5907e5c5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explaining Information in a Image for CNN:\n",
    "    a. Explain how Convolutional Neural Networks (CNN) can extract and understand information\n",
    "        from an image. Discuss the key components and processes involved in analyzing image data\n",
    "        using CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a35295-d585-4c7d-9568-3fe0ac9cccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans: Convolutional Neural Networks (CNNs) are a class of deep learning models specifically designed for \n",
    "     processing and analyzing visual data, such as images. CNNs excel at extracting and understanding \n",
    "     information from images due to their unique architecture, which consists of several key components and processes:\n",
    "    \n",
    "    1. Convolutional Layers:\n",
    "            Convolutional layers are the core building blocks of CNNs. These layers apply a set of learnable\n",
    "            filters (also known as kernels) to the input image through a process called convolution. Each filter \n",
    "            extracts specific features from the input image, such as edges, textures, or patterns, by performing \n",
    "            element-wise multiplication and aggregation operations. Convolutional layers help capture hierarchical\n",
    "            representations of visual features, with early layers detecting low-level features like edges and \n",
    "            gradients, while deeper layers detect more complex features.\n",
    "            \n",
    "    2.Pooling Layers:\n",
    "            Pooling layers are often inserted between convolutional layers to reduce the spatial dimensions of \n",
    "            feature maps while retaining important information. The most common pooling operation is max pooling, \n",
    "            which extracts the maximum value from a local region of the input feature map. Pooling helps improve \n",
    "            computational efficiency, reduces overfitting, and enhances the model's translation invariance by \n",
    "            aggregating relevant information from neighboring pixels.\n",
    "            \n",
    "    3. Activation Functions:\n",
    "            Activation functions introduce non-linearity to the CNN model, enabling it to learn complex mappings\n",
    "            between input and output data. Popular activation functions used in CNNs include ReLU (Rectified Linear\n",
    "            Unit), which introduces non-linearities by thresholding the input values, and variants like Leaky ReLU \n",
    "            and ELU. Activation functions help CNNs capture complex patterns and relationships within the image data.\n",
    "            \n",
    "    4. Fully Connected Layers:\n",
    "            Fully connected layers are typically placed at the end of the CNN architecture to perform classification\n",
    "            or regression tasks based on the extracted features. These layers connect every neuron in one layer to\n",
    "            every neuron in the next layer, allowing the network to learn high-level representations of the input \n",
    "            data. Fully connected layers combine the spatial information extracted by convolutional and pooling\n",
    "            layers to make predictions about the input image's class or category.\n",
    "            \n",
    "    5. Training Process:\n",
    "            CNNs are trained using large datasets of labeled images through a process called backpropagation. \n",
    "            During training, the model adjusts its parameters (weights and biases) based on the error between \n",
    "            predicted and actual labels, using optimization algorithms like stochastic gradient descent (SGD) \n",
    "            or Adam. The objective is to minimize the loss function, such as categorical cross-entropy for \n",
    "            classification tasks, by iteratively updating the network's parameters. Through this process, \n",
    "            CNNs learn to extract relevant features from images and make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f3715-c5a5-47fd-8285-49796e173211",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Flattening Images for ANN:\n",
    "    a. Discuss why it is not recommended to flatten images directly and input them into an\n",
    "        Artificial Neural Network (ANN) for image classification. Highlight the limitations and\n",
    "        challenges associated with this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2f198-a47d-4003-b120-d30772f8e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Flattening images and directly inputting them into an Artificial Neural Network (ANN) for image \n",
    "      classification is not recommended due to several limitations and challenges associated with this approach:\n",
    "    \n",
    "    1. Loss of Spatial Information:\n",
    "        Flattening an image collapses its multi-dimensional structure into a single vector of pixel values, \n",
    "        disregarding the spatial relationships between neighboring pixels. By doing so, important spatial \n",
    "        information such as shapes, textures, and object layouts is lost. ANN models lack the capability to \n",
    "        capture and exploit this spatial information effectively, which can lead to reduced performance in image \n",
    "        classification tasks.\n",
    "        \n",
    "    2. Excessive Number of Parameters:\n",
    "        Flattening large images results in input vectors with a high dimensionality, leading to a correspondingly \n",
    "        large number of parameters in the subsequent layers of the neural network. This increases the computational \n",
    "        complexity and memory requirements of the model, making it challenging to train and optimize effectively. \n",
    "        Moreover, ANNs may struggle to learn meaningful representations from high-dimensional input spaces, leading \n",
    "        to overfitting and poor generalization performance.\n",
    "        \n",
    "    3. Inefficiency in Capturing Local Patterns:\n",
    "        ANNs lack the inherent ability to capture local patterns and spatial dependencies present in images. Since\n",
    "        each neuron in the network is connected to every neuron in the preceding layer, ANNs treat all input features \n",
    "        as independent, disregarding the local correlations and structures inherent in visual data. This inefficiency \n",
    "        hampers the network's ability to learn hierarchical representations of image features, limiting its performance \n",
    "        in tasks requiring fine-grained analysis of visual content.\n",
    "        \n",
    "    4. Limited Translation Invariance:\n",
    "        ANNs typically lack translation invariance, meaning they cannot recognize objects regardless of their position \n",
    "        or orientation within the image. Flattening images removes spatial information critical for capturing \n",
    "        translation-invariant features, making the model sensitive to variations in object position, scale, and \n",
    "        orientation. This limitation can degrade the model's robustness and performance, particularly in tasks where\n",
    "        object appearance varies across different regions of the image.\n",
    "        \n",
    "    5. Difficulty in Handling Variable-sized Inputs:\n",
    "        Flattening images results in fixed-size input vectors, making it challenging to handle images of variable \n",
    "        sizes or aspect ratios. ANNs require input data with a consistent shape, which necessitates preprocessing \n",
    "        steps such as resizing or cropping images to a predefined size. This preprocessing introduces additional \n",
    "        complexities and potential information loss, reducing the model's ability to generalize across diverse\n",
    "        image datasets.\n",
    "    \n",
    "  while ANNs are capable of learning complex mappings between input and output data, their limitations in handling\n",
    "  spatial information and capturing local patterns make them suboptimal for image classification tasks. Instead, \n",
    "  specialized architectures like Convolutional Neural Networks (CNNs) are better suited for processing and analyzing \n",
    "  image data, as they are specifically designed to exploit the spatial structure and hierarchical features present in\n",
    "  images, leading to superior performance in visual recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505cb2d-7d00-4737-8e1f-adac4ba24760",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Applying CNN to the MNIST Dataset:\n",
    "    a. Explain why it is not necessary to apply CNN to the MNIST dataset for image classification.\n",
    "       Discuss the characteristics of the MNIST dataset and how it aligns with the requirements of\n",
    "       CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af0e3d-2f17-484a-9c69-1f84597e77a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Applying Convolutional Neural Networks (CNNs) to the MNIST dataset for image classification is \n",
    "      not necessary due to the dataset's simplicity and characteristics, which can be effectively handled\n",
    "      by simpler models like Multi-Layer Perceptrons (MLPs). The MNIST dataset consists of grayscale images \n",
    "      of handwritten digits (0 to 9), each of size 28x28 pixels. Here's why CNNs may not be required for the \n",
    "      MNIST dataset:\n",
    "    \n",
    "    1. Simple Spatial Structure:\n",
    "        MNIST images are relatively small (28x28 pixels) and contain simple spatial structures compared to \n",
    "        more complex images found in real-world datasets. The digits are centered and occupy a significant \n",
    "        portion of the image, with minimal background noise or clutter. The simplicity of the spatial structure\n",
    "        allows simpler models like MLPs to effectively learn and recognize patterns without the need for \n",
    "        specialized convolutional operations.\n",
    "        \n",
    "    2. Lack of Local Patterns:\n",
    "        MNIST digits lack intricate local patterns and spatial dependencies typically present in natural images.\n",
    "        Each pixel in the image contributes directly to the representation of the digit, without complex interactions \n",
    "        or hierarchical features. As a result, the benefits of convolutional operations, such as feature extraction \n",
    "        and spatial hierarchies, may not be fully utilized in the context of the MNIST dataset.\n",
    "        \n",
    "    3. Low Dimensionality:\n",
    "        The MNIST dataset has relatively low-dimensional input data, with each image containing only 28x28 = 784\n",
    "        pixels. As a result, MLPs can efficiently process the entire image as a flattened input vector, without \n",
    "        the need for specialized convolutional and pooling layers. The low dimensionality of the input space reduces \n",
    "        the risk of overfitting and computational complexity, making simpler models more suitable for the task.\n",
    "        \n",
    "    4.Homogeneity of Features:\n",
    "        MNIST digits exhibit a high degree of homogeneity in terms of shape, size, and appearance. The variability \n",
    "        among different digits is relatively low compared to more diverse datasets. This homogeneity simplifies the\n",
    "        classification task, as MLPs can learn to distinguish between digit classes based on global patterns and pixel \n",
    "        intensities without requiring complex spatial hierarchies or feature maps.\n",
    "        \n",
    " While CNNs can certainly be applied to the MNIST dataset and may achieve marginally better performance through feature\n",
    "extraction and hierarchical learning, the dataset's simplicity and characteristics align well with the capabilities of \n",
    "simpler models like MLPs. Therefore, using CNNs for MNIST classification may be considered over-engineering, as the\n",
    "additional complexity and computational overhead may not provide significant improvements in accuracy compared to simpler approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b480a69-d00d-480b-9ffc-80ac2c6057ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Extracting Features at Local Space:\n",
    "    a. Justify why it is important to extract features from an image at the local level rather than\n",
    "        considering the entire image as a whole. Discuss the advantages and insights gained by\n",
    "        performing local feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81a5b5-8372-46ec-b837-f5191e7c33c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans: It is important to extract features from an image at the local level rather than considering the entire image as \n",
    "     a whole because local feature extraction allows for capturing fine-grained details, patterns, and spatial\n",
    "     relationships that may not be apparent at the global level. Here are several reasons why performing local\n",
    "     feature extraction is advantageous:\n",
    "    \n",
    "    1. Enhanced Discriminative Power:\n",
    "        Local feature extraction enables the identification of discriminative patterns and structures within \n",
    "        specific regions of an image. By focusing on local neighborhoods or patches, the model can capture \n",
    "        intricate details and variations that contribute to distinguishing between different objects or classes. \n",
    "        This enhances the discriminative power of the feature representation, leading to improved classification performance.\n",
    "        \n",
    "    2. Translation Invariance:\n",
    "        Local feature extraction facilitates translation invariance, meaning that the model can recognize patterns \n",
    "        and objects regardless of their position or orientation within the image. By extracting features from local \n",
    "        regions and aggregating them across the image, the model becomes less sensitive to variations in object location, \n",
    "        scale, or orientation. This robustness is crucial for tasks where objects may appear in different contexts or\n",
    "        positions within the image.\n",
    "        \n",
    "    3. Hierarchical Representation:\n",
    "        Local feature extraction enables the construction of hierarchical representations of visual data, where features \n",
    "        at different levels of abstraction are learned progressively. By extracting low-level features from local regions \n",
    "        and combining them to form higher-level representations, the model can capture complex structures and relationships\n",
    "        within the image. This hierarchical approach allows for the extraction of meaningful features that capture both local \n",
    "        and global context.\n",
    "    \n",
    "    4. Robustness to Noise and Occlusions:\n",
    "        Local feature extraction helps improve the robustness of the model to noise, occlusions, and other forms of\n",
    "        image corruption. By focusing on local regions, the model can effectively filter out irrelevant information \n",
    "        and detect relevant features even in the presence of partial occlusions or background clutter. This robustness \n",
    "        ensures that the model can generalize well to unseen data and handle real-world variations and challenges.\n",
    "        \n",
    "    5. Efficient Computation:\n",
    "        Local feature extraction reduces the computational complexity of the model by focusing computational resources \n",
    "        on relevant image regions. Instead of processing the entire image at once, the model can selectively extract \n",
    "        features from local patches, reducing the number of parameters and computations required. This efficiency is \n",
    "        especially important for large-scale image datasets and real-time applications where computational resources are limited.\n",
    "        \n",
    "  performing local feature extraction allows for capturing fine-grained details, robustness to variations, hierarchical\n",
    "  representation, and efficient computation, leading to improved performance and generalization in various computer vision \n",
    "    tasks such as object recognition, detection, and segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27dedee-2b3a-47c2-ae3f-c1e5afc11b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "̧8. Importance of Convolution ad Max Pooling:\n",
    "    a. Elaborate on the importance of convolution and max pooling operations in a Convolutional\n",
    "        Neural Network (CNN). Explain how these operations contribute to feature extraction and\n",
    "        spatial down-sampling in CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e3df9-5295-490a-ad29-fd05d2f9756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Convolution and max pooling operations are fundamental components of Convolutional Neural Networks (CNNs), \n",
    "      playing crucial roles in feature extraction and spatial down-sampling. Here's how each operation contributes \n",
    "      to the overall functionality of CNNs:\n",
    "\n",
    "        1. Convolution Operation:\n",
    "            Convolution is a mathematical operation that involves applying a filter (also known as a kernel) to an \n",
    "            input image. The filter slides over the input image, computing the element-wise multiplication between\n",
    "            its values and the corresponding pixel values in the local receptive field of the input. The result of \n",
    "            this operation is then summed to produce a single output value at each spatial position, forming a feature map.\n",
    "            \n",
    "            Importance in Feature Extraction:\n",
    "                - Convolution operations act as feature detectors, capturing localized patterns and structures within the \n",
    "                  input image. By learning filters that highlight specific features like edges, textures, or gradients, CNNs\n",
    "                  can extract meaningful representations of visual information.\n",
    "                - Through the training process, CNNs learn to adapt the filter weights to detect relevant features that are\n",
    "                  informative for the task at hand, such as object recognition or segmentation.\n",
    "                \n",
    "        2. Max Pooling Operation:\n",
    "            Max pooling is a downsampling operation that partitions the input feature map into non-overlapping regions and \n",
    "            retains only the maximum value within each region. This process reduces the spatial dimensions of the feature \n",
    "            map while preserving the most salient information.\n",
    "            \n",
    "            Importance in Spatial Down-sampling:\n",
    "                - Max pooling helps reduce the computational complexity of the model and prevents overfitting by reducing \n",
    "                  the number of parameters and the spatial resolution of the feature maps.\n",
    "                - By discarding non-maximal values and retaining only the strongest activations within each region, max \n",
    "                  pooling emphasizes the most relevant features while suppressing noise and irrelevant variations in the input.\n",
    "                - Spatial down-sampling achieved through max pooling introduces translation invariance, allowing the model \n",
    "                  to capture higher-level abstract representations that are invariant to small shifts in the input.\n",
    "        \n",
    "        Overall, convolution and max pooling operations work in tandem within CNNs to extract informative features from the \n",
    "        input data while reducing its spatial dimensions. Through feature extraction and spatial down-sampling, CNNs can \n",
    "        effectively capture hierarchical representations of visual information, enabling tasks such as image classification, \n",
    "        object detection, and segmentation. These operations play crucial roles in the success and effectiveness of CNN \n",
    "        architectures in various computer vision applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
