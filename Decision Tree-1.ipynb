{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41f77a4-75b4-4473-8350-1708af725dfd",
   "metadata": {},
   "source": [
    "Q1. Describe the decision tree classifier algorithm and how it works to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f225a-815b-4ffd-a4b5-100f05207dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : A Decision Tree Classifier is a supervised machine learning algorithm used primarily for classification tasks, although \n",
    "      it can also be adapted for regression tasks. It works by recursively splitting the dataset into subsets based on the most \n",
    "      significant attribute or feature at each level of the tree.\n",
    "      \n",
    "      Here's a step-by-step description of how the Decision Tree Classifier algorithm works:\n",
    "      \n",
    "      1. Data Splitting: \n",
    "         a. The algorithm starts with the entire dataset, which contains labeled examples (instances with known class labels).\n",
    "         b. At each level or node of the tree, the algorithm selects the feature that provides the best split, aiming to create subsets \n",
    "            of data that are more homogeneous in terms of class labels.\n",
    "    \n",
    "     2. Feature Selection\n",
    "        a. For each attribute or feature in the dataset, the algorithm evaluates its ability to split the data effectively. This \n",
    "           is typically done using metrics such as Gini Impurity, Information Gain, or Gain Ratio.\n",
    "        b. The feature with the highest score (i.e., the most significant predictor) is chosen as the splitting criterion for the current node.\n",
    "        \n",
    "     3. Splitting Criteria \n",
    "        a . The selected feature is used to divide the data into subsets based on its values. Each subset corresponds to a specific branch \n",
    "            or child node in the decision tree.\n",
    "        b. For categorical features, the dataset is partitioned into subsets, each corresponding to one unique category.\n",
    "        c. For numerical features, the algorithm determines a threshold value to create two subsets, one with values less than the \n",
    "           threshold and another with values greater than or equal to the threshold.\n",
    "           \n",
    "     4. Recursion:\n",
    "        a. The above steps are repeated recursively for each child node, creating a binary tree structure.\n",
    "        b. The algorithm continues splitting the data until one or more stopping criteria are met. These criteria may include a\n",
    "           predefined depth limit, a minimum number of samples in a node, or until all instances in a node belong to the same class.\n",
    "           \n",
    "    5. Leaf Nodes and Class Assignment:\n",
    "       a. When a stopping criterion is met, the current node becomes a leaf node. Leaf nodes do not split further and represent \n",
    "          the predicted class for instances that reach them.\n",
    "       b. The class assigned to a leaf node is typically determined by majority voting. That is, the class label assigned to the\n",
    "          leaf node is the most frequent class label among the instances in that node.\n",
    "          \n",
    "    6. Prediction:\n",
    "       a.  To make predictions for new, unseen instances, the algorithm traverses the decision tree from the root node to a leaf node.\n",
    "       b . At each internal node, it follows the branch corresponding to the attribute value of the instance being evaluated.\n",
    "       c. Once it reaches a leaf node, it assigns the class label associated with that leaf node as the predicted class for the input instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e9d76-ef54-4e08-a1c1-99906c4b447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051c3c4-528a-488d-9f15-b7b7e59cf20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : 1. Entropy:\n",
    "        a. Entropy measures the impurity or disorder of a node in terms of the class distribution. A lower entropy indicates a more \n",
    "           pure node where all instances belong to a single class.\n",
    "           \n",
    "        b. Entropy (H(S)) is calculated as\n",
    "                H(S) = - P(+) log2 P(+) - P(-) log2 P(-) \n",
    "          \n",
    "\n",
    "     2. Gini Impurity:\n",
    "     \n",
    "         a. Gini Impurity measures the likelihood of an incorrect classification if a random sample from the node were \n",
    "            classified randomly according to the class distribution.\n",
    "         b. For a given node with classes C1 ,C2 ,…,Ck and class probabilities p1 ,p2 ,…,pk , the Gini Impurity (Gini) is calculated as follows:\n",
    "                       k\n",
    "             Gini= 1 − ∑ (pi)^2\n",
    "                       i=1\n",
    "     3. Information Gain:        \n",
    "         a. Information Gain measures the reduction in entropy (or impurity) achieved by splitting a node based on a particular attribute. \n",
    "            It quantifies how much uncertainty in class labels is reduced after the split.\n",
    "         b. For a parent node with entropy H(D) and child nodes with entropies H(Di) after the split, the Information Gain (IG) is given by:\n",
    "                            \n",
    "                            m   |SV|\n",
    "             IG  =  H(D) − ∑    ----   H (SV)\n",
    "                           i=1   |S|   \n",
    "                                  \n",
    "     4. Gain Ratio : Gain Ratio is an improvement over Information Gain that takes into account the intrinsic information in an attribute,\n",
    "                     which avoids overfitting by favoring attributes with fewer values \n",
    "                            IG\n",
    "                     GR =  ----\n",
    "                            IV\n",
    "     5. Splitting Criteria:\n",
    "\n",
    "        a. The decision tree algorithm selects the attribute that maximizes Information Gain or Gain Ratio to split the data at each node.\n",
    "        b. The attribute chosen for the split becomes the root of a subtree, and the process recurs for each child node.\n",
    "        \n",
    "    6. Recursive Splitting:\n",
    "\n",
    "        a. The algorithm recursively splits the data into subsets based on the selected attribute.\n",
    "        b. At each level, the process continues until a stopping criterion is met, such as reaching a predefined depth or having a minimum \n",
    "           number of instances in a node.\n",
    "        \n",
    "    7. Leaf Node Assignment:\n",
    "           Once the splitting process reaches a stopping criterion, the instances in a node are assigned to a majority class label, \n",
    "           creating a leaf node.\n",
    "    8. Prediction:\n",
    "          For a new instance, the decision tree is traversed from the root to a leaf node based on attribute values, and the majority class \n",
    "        label of the leaf node becomes the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce21ed-d0cd-403a-a48b-88e9a5383166",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Explain how a decision tree classifier can be used to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffdf4fb-c8e1-4e71-964f-a230d5e41001",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Step 1: Data Preparation:\n",
    "                  Begin with a dataset that includes instances, each labeled as belonging to one of two classes, often \n",
    "                  denoted as \"positive\" and \"negative,\" \"yes\" and \"no,\" or \"1\" and \"0.\"\n",
    "        \n",
    "    Step 2: Feature Selection:\n",
    "                 Select the features (attributes) from the dataset that are relevant for the classification task. These\n",
    "                 features will be used to make decisions at each node of the decision tree.\n",
    "    \n",
    "    Step 3: Building the Decision Tree:\n",
    "                The decision tree is constructed recursively, starting with the entire dataset at the root node.\n",
    "                \n",
    "    Step 4: Node Splitting:\n",
    "                1. At each internal node of the tree, the algorithm selects the feature that provides the best split, aiming to \n",
    "                   create subsets of data that are more homogeneous with respect to the class labels.\n",
    "                2.The algorithm uses a splitting criterion to evaluate the quality of each possible split. Common criteria include\n",
    "                  Gini Impurity, Information Gain, or Gain Ratio.\n",
    "    \n",
    "    Step 5: Recursive Splitting:\n",
    "                1. The dataset is divided into two subsets based on the selected feature's values: one subset contains instances \n",
    "                   where the feature meets the condition (e.g., \"Age > 30\"), and the other contains instances that do not meet the condition.\n",
    "                2. The algorithm repeats the splitting process for each child node, creating a binary tree structure. It continues this \n",
    "                   process until a stopping criterion is met. Common stopping criteria include reaching a maximum depth, having a minimum \n",
    "                   number of instances in a node, or achieving perfect purity (all instances in a node belong to the same class).\n",
    "                   \n",
    "    Step 6: Leaf Node Assignment:\n",
    "                1. When a stopping criterion is met, the current node becomes a leaf node. Leaf nodes do not split further and represent \n",
    "                   a predicted class label for instances that reach them.\n",
    "                2. Typically, the class label assigned to a leaf node is determined by majority voting. The class label with the highest\n",
    "                   frequency among the instances in that node is assigned as the predicted class.\n",
    "                   \n",
    "    Step 7: Prediction\n",
    "                To make predictions for new, unseen instances, the decision tree is traversed from the root node to a leaf node based on \n",
    "                the attribute values of the instance being evaluated.\n",
    "        \n",
    "    Step 8: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd3544-eb58-4355-b590-cbfc5aaabb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
    "    predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc190f-a9f8-47fd-ab24-425d25d08fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : The geometric intuition behind decision tree classification involves visualizing how a decision tree partitions the feature \n",
    "      space into regions corresponding to different classes. This visualization helps understand how decision trees work and make predictions.\n",
    "      \n",
    "      Geometric Intuition:\n",
    "    \n",
    "       1.Binary Splitting: At each internal node of the decision tree, the algorithm selects a feature and a threshold value that best\n",
    "         separates the data into two subsets based on class labels. This is akin to drawing a decision boundary, which divides the feature\n",
    "         space into two regions.\n",
    "        \n",
    "      2.Recursive Partitioning: The process of binary splitting is repeated recursively for each subset, leading to further partitioning. \n",
    "         This results in a hierarchical structure of decision boundaries, forming regions or subspaces in the feature space\n",
    "    \n",
    "     3.Leaf Nodes: When the algorithm reaches a stopping criterion (e.g., a maximum depth or minimum samples per leaf), it assigns a \n",
    "       class label to each leaf node based on the majority class within that region. This can be seen as labeling each partitioned \n",
    "       region with a class.\n",
    "       \n",
    "    Using Decision Trees for Predictions:\n",
    "            Once the decision tree is constructed, it can be used to make predictions for new data points:\n",
    "         1.Traversal: Start at the root node and evaluate the feature values of the new data point.\n",
    "         2.Branching: Follow the path through the tree by comparing the feature values to the splitting \n",
    "           criteria at each internal node. Move left or right through the tree based on whether the condition is met.\n",
    "         3.Leaf Node: When you reach a leaf node, the class label associated with that leaf node becomes the predicted\n",
    "           class for the input data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5382bd7-8aed-4855-b695-f01af931ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
    "    classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672040e0-b6fe-468a-8594-af91853a8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : A confusion matrix is a table used in classification analysis to evaluate the performance of a machine \n",
    "      learning model, particularly in binary classification but also in multi-class classification. It provides a detailed \n",
    "      summary of how well a model's predictions align with the actual class labels in a dataset. The confusion matrix is \n",
    "      especially useful for understanding the types and frequencies of classification errors made by a model.\n",
    "      \n",
    "      A confusion matrix consists of four key metrics, which are calculated based on the model's predictions and the actual class labels:\n",
    "          1.True Positives (TP): The number of instances that were correctly predicted as positive (belonging to the positive class).\n",
    "          2. True Negatives (TN): The number of instances that were correctly predicted as negative (belonging to the negative class).\n",
    "          3. False Positives (FP): The number of instances that were incorrectly predicted as positive when they actually belong to the\n",
    "            negative class. Also known as a Type I error.\n",
    "          4.False Negatives (FN): The number of instances that were incorrectly predicted as negative when they actually belong to \n",
    "            the positive class. Also known as a Type II error.\n",
    "            \n",
    "    The confusion matrix provides valuable information about a classification model's performance, which can be used to calculate various\n",
    "    evaluation metrics, including:\n",
    "    \n",
    "    1 . Accuracy: The overall proportion of correct predictions made by the model. It is calculated as (TP + TN) / (TP + TN + FP + FN). \n",
    "    2. Precision: Also known as Positive Predictive Value (PPV), it measures the proportion of true positive predictions out of all positive \n",
    "       predictions made by the model. It is calculated as TP / (TP + FP).\n",
    "    3. Recall: Also known as Sensitivity or True Positive Rate (TPR), it measures the proportion of true positive predictions out of all \n",
    "       actual positive instances. It is calculated as TP / (TP + FN).\n",
    "    4. F1-Score: The harmonic mean of precision and recall, which balances the trade-off between precision and recall. It is calculated\n",
    "        as 2 * (Precision * Recall) / (Precision + Recall).\n",
    "    5. Specificity: Measures the proportion of true negative predictions out of all actual negative instances. It is calculated as TN / (TN + FP).\n",
    "    6. False Positive Rate (FPR): Measures the proportion of false positive predictions out of all actual negative instances. \n",
    "        It is calculated as FP / (TN + FP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e1d8a-0fa1-429d-b65e-994d20111318",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "    calculated from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346c135-7f5b-4e8a-bde7-a653f2c511ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Suppose we have built a binary classification model to predict whether an email is spam (positive class) or not\n",
    "      spam (negative class). We evaluate the model on a test dataset, and the confusion matrix looks like this:\n",
    "      \n",
    "                         Predicted Spam     Predicted Not Spam\n",
    "        Actual Spam        90 (TP)                10 (FN)\n",
    "        Actual Not Spam    5 (FP)                895 (TN)\n",
    "        \n",
    "    1. Precision: Precision measures the accuracy of positive predictions. It answers the question: \"Of all the emails \n",
    "                  predicted as spam, how many were actually spam?\"\n",
    "                 \n",
    "                 Precision = TP / (TP + FP) = 90 / (90 + 5) = 90 / 95 ≈ 0.947\n",
    "          So, the precision of the model is approximately 0.947, indicating that about 94.7% of the emails predicted as spam were indeed spam.\n",
    "     \n",
    "    2. Recall: Recall measures the model's ability to correctly identify all relevant instances. It answers the question:\n",
    "               \"Of all the actual spam emails, how many were correctly predicted as spam?\"\n",
    "\n",
    "                    Recall = TP / (TP + FN) = 90 / (90 + 10) = 90 / 100 = 0.9\n",
    "            The recall of the model is 0.9, which means it correctly identified 90% of the actual spam emails.\n",
    "            \n",
    "    3.F1 Score: The F1 score is the harmonic mean of precision and recall and is used to balance the trade-off between \n",
    "                precision and recall. It provides a single metric that considers both false positives and false negatives.\n",
    "\n",
    "                F1 Score = 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.947 * 0.9) / (0.947 + 0.9) ≈ 0.923\n",
    "                 The F1 score is approximately 0.923."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110605f6-1f8d-4209-ac0b-e6be6d7b2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\n",
    "    explain how this can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae370ac0-34e3-445d-826c-bb2d9348390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Importance of Choosing the Right Metric\n",
    "      \n",
    "      1. Alignment with Objectives: Different classification tasks have different objectives. For example, in a medical \n",
    "         diagnosis task, correctly identifying positive cases (e.g., diseases) might be more critical than overall accuracy.\n",
    "         The choice of metric should reflect the task's primary objective.\n",
    "      2.Handling Imbalanced Data: Imbalanced datasets, where one class significantly outnumbers the other, are common in \n",
    "        classification. In such cases, accuracy may not be a suitable metric as it can be misleading. Metrics like precision,\n",
    "        recall, F1 score, and area under the ROC curve (AUC-ROC) can provide a more balanced view of performance.\n",
    "        \n",
    "        \n",
    "        How to Choose the Right Metric:\n",
    "        \n",
    "        1. Understand Your Problem: Start by gaining a deep understanding of the specific classification problem you are tackling.\n",
    "            Consider the domain, the consequences of different types of errors, and the primary goals.\n",
    "\n",
    "        2. Consider Imbalance: Examine the class distribution in your dataset. If there's a significant class imbalance, \n",
    "           prioritize metrics like precision, recall, F1 score, or AUC-ROC, as they can provide more meaningful insights than accuracy alone.\n",
    "        \n",
    "        3. Business and Stakeholder Requirements: Discuss the project with stakeholders and gather their input. Understand their\n",
    "           expectations and which aspects of the model's performance are most critical to them. This dialogue can help you identify\n",
    "           the most relevant metrics.\n",
    "           \n",
    "        4.Select Metrics for Your Task:\n",
    "\n",
    "            Accuracy: Use when classes are balanced, and all types of errors are equally important.\n",
    "            Precision and Recall: Useful when the cost of false positives and false negatives is different. \n",
    "                                  Precision emphasizes minimizing false positives, while recall emphasizes minimizing false negatives.\n",
    "            F1 Score: A balance between precision and recall, suitable for imbalanced datasets.\n",
    "            AUC-ROC: Useful for evaluating binary classifiers, especially when class separation is important.\n",
    "            AUC-PR (Area Under the Precision-Recall Curve): Appropriate when dealing with highly imbalanced datasets.\n",
    "            Custom Cost-Sensitive Metrics: If specific costs are associated with different types of errors, create custom evaluation\n",
    "            metrics that consider these costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcba53-7add-48f7-a6e0-0f48bfa7c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
    "    explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5738564-bc54-4b90-8430-f75797b04d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : An example of a classification problem where precision is the most important metric is in the context of email spam detection.\n",
    "        In email spam detection, the primary goal is to accurately classify incoming emails as either \"spam\" or \"not spam\" (ham) to\n",
    "        protect users from unwanted and potentially harmful messages. In this scenario, precision is crucial for the following reasons:\n",
    "        \n",
    "        Reasons for Prioritizing Precision:\n",
    "            1. User Experience: False positives, where legitimate emails are incorrectly classified as spam, can have a significant \n",
    "               negative impact on user experience. Users may miss important emails, such as work-related messages, personal communications, \n",
    "               or notifications, if their emails are marked as spam. High precision reduces the likelihood of these false positives.\n",
    "            2. Trust and Credibility: Users need to trust their email spam filters to accurately identify and filter out spam. If the spam \n",
    "              filter generates too many false positives, users may lose trust in the system and become less likely to rely on it, leading \n",
    "              to potential security risks.\n",
    "            3. Legal and Compliance Issues: In some contexts, misclassifying emails as spam can have legal and compliance implications.\n",
    "               For example, financial institutions need to ensure that important regulatory and customer communications are not erroneously\n",
    "               marked as spam.\n",
    "            4.Resource Efficiency: Reducing false positives also leads to resource efficiency. Spam filters typically require manual \n",
    "              review and correction of false positives. Minimizing these corrections saves time and resources for both users and system\n",
    "              administrators.\n",
    "            5. Impact on Reputation: If a user or organization consistently sends emails that are falsely marked as spam, it can\n",
    "               negatively impact their email sender reputation. Email service providers may classify them as spammers, which can affect \n",
    "               their ability to deliver emails to recipients' inboxes.\n",
    "               \n",
    "        Example Scenario:\n",
    "\n",
    "        Let's consider an example scenario where precision is critical in email spam detection:\n",
    "\n",
    "        Suppose you are developing a spam filter for a corporate email system used by a large organization. It's crucial to ensure \n",
    "        that important internal communications, including project updates, meeting invitations, and sensitive information, are not \n",
    "        mistakenly classified as spam. In this context:\n",
    "        \n",
    "        True Positives (TP): Emails correctly classified as spam and filtered out are essential for protecting users from actual spam.\n",
    "\n",
    "        False Positives (FP): Emails incorrectly classified as spam (false positives) could lead to missed opportunities, delayed \n",
    "         responses, or overlooked critical information. For instance, missing a meeting invitation due to a false positive could\n",
    "          have a direct negative impact on productivity and collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f5472-540f-4dd9-b9b0-e60246bcd677",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. Provide an example of a classification problem where recall is the most important metric, and explain\n",
    "    why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846d3bf-a2b3-4fa5-83d9-bc37eb8dbe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : An example of a classification problem where recall is the most important metric is in the context of medical\n",
    "      testing for a rare and life-threatening disease. In such scenarios, correctly identifying all true positive cases\n",
    "      (minimizing false negatives) is of paramount importance. Here's why recall takes precedence in this context:\n",
    "      \n",
    "      Reasons for Prioritizing Recall:\n",
    "\n",
    "        1. Patient Health and Safety: In medical diagnosis, especially for severe diseases, the primary concern is the health and safety of\n",
    "           the patients. Missing a positive case (false negative) can have dire consequences, as it means that a patient with the disease\n",
    "           will not receive timely treatment, leading to potentially severe health issues or even death.\n",
    "\n",
    "        2. Early Intervention: Timely detection and intervention are critical for many medical conditions. For diseases where early \n",
    "           treatment significantly improves prognosis, maximizing recall ensures that as many cases as possible are identified early.\n",
    "\n",
    "        3. Public Health: In the case of contagious diseases, missing a positive case can have public health implications. Containment \n",
    "           and prevention strategies often rely on identifying and isolating individuals with the disease promptly.\n",
    "           \n",
    "        4.Minimizing Legal and Ethical Issues: Inaccurate diagnoses can lead to legal and ethical challenges for healthcare providers.\n",
    "          Missing a positive case may result in legal liability and damage to the reputation of healthcare institutions.\n",
    "          \n",
    "          \n",
    "    Example Scenario:\n",
    "\n",
    "        Let's consider a specific example to illustrate the importance of recall in medical diagnosis:\n",
    "\n",
    "        Disease: Early-Stage Ovarian Cancer Detection\n",
    "\n",
    "        1.Ovarian cancer is often referred to as the \"silent killer\" because it tends to exhibit minimal or nonspecific \n",
    "          symptoms until it reaches an advanced stage.\n",
    "        2.Early detection of ovarian cancer is challenging but can significantly improve a patient's chances of survival\n",
    "          and successful treatment.\n",
    "        3.Imagine a machine learning model designed to assist in the diagnosis of early-stage ovarian cancer based on various\n",
    "          medical tests and imaging data.\n",
    "          \n",
    "        In this scenario:\n",
    "\n",
    "        1. True Positives (TP): These are cases where the model correctly identifies individuals with early-stage ovarian cancer. \n",
    "           Ensuring a high number of true positives is crucial because it means that patients with the disease are detected and \n",
    "            can receive treatment promptly.\n",
    "\n",
    "        2. False Negatives (FN): False negatives are cases where individuals have early-stage ovarian cancer but are not identified\n",
    "           by the model. Missing even one case could lead to delayed treatment and potentially adverse outcomes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
