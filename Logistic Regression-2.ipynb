{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b992626f-8b41-4e06-97d8-eb923fbf6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c80022-1abd-4644-a8d0-de452f3d5cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Grid Search Cross-Validation (GridSearchCV) is a hyperparameter tuning technique used in machine learning to systematically search\n",
    "      through a predefined hyperparameter space and find the combination of hyperparameters that yields the best model performance.\n",
    "      Hyperparameters are values that are set before training a machine learning model and can significantly influence the model's \n",
    "      behavior and performance. GridSearchCV automates the process of selecting the optimal hyperparameters by exhaustively searching \n",
    "      through all possible combinations within a specified grid of hyperparameter values.\n",
    "        \n",
    "        Purpose of GridSearchCV:\n",
    "\n",
    "            The main purpose of GridSearchCV is to find the hyperparameter values that result in the best model performance on a validation set.\n",
    "            This process is crucial because selecting appropriate hyperparameters can significantly impact a model's ability to generalize well\n",
    "            to new, unseen data. GridSearchCV helps prevent manual trial-and-error and ensures a more systematic approach to hyperparameter tuning.\n",
    "            \n",
    "    How GridSearchCV Works:\n",
    "\n",
    "            1. Define Hyperparameter Grid: Specify a set of hyperparameters along with the possible values for each hyperparameter.\n",
    "                                           This defines the grid of hyperparameter combinations that will be searched.\n",
    "\n",
    "            2. Define Scoring Metric: Choose a performance metric (e.g., accuracy, F1-score, AUC) that will be used to evaluate and \n",
    "                                      compare different hyperparameter combinations.\n",
    "\n",
    "            3. Cross-Validation: For each combination of hyperparameters in the grid, perform k-fold cross-validation on the training data. \n",
    "                                  This involves splitting the training data into k subsets (folds), using k-1 folds for training and the\n",
    "                                  remaining fold for validation. This process is repeated k times, with each fold serving as the validation\n",
    "                                  set once.\n",
    "\n",
    "            4. Model Training and Evaluation: Train a model using each hyperparameter combination on the training folds and evaluate its\n",
    "                                              performance on the corresponding validation fold using the chosen scoring metric.\n",
    "                \n",
    "            5.Select Best Hyperparameters: Calculate the average score across all cross-validation folds for each hyperparameter combination. \n",
    "                                           The combination that yields the highest average score is selected as the optimal set of hyperparameters.\n",
    "\n",
    "            6. Final Model: Train a final model using the selected optimal hyperparameters on the entire training dataset. This model is then \n",
    "                            evaluated on an independent test dataset to estimate its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de40f62-f4a9-42b3-baf8-9e83fefcb1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1baacd-d821-4c2b-93f5-95c4be3737c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Grid Search Cross-Validation (GridSearchCV):\n",
    "\n",
    "            1.Search Strategy: GridSearchCV exhaustively searches through all possible combinations of hyperparameters specified\n",
    "                               in a predefined grid.\n",
    "            2.Exploration Method: It explores the entire hyperparameter space systematically, evaluating each combination\n",
    "                                  using cross-validation.\n",
    "            3.Computationally Intensive: GridSearchCV can be computationally intensive, especially when the hyperparameter space is \n",
    "                                         large or when multiple hyperparameters need to be tuned simultaneously.\n",
    "            4.Full Exploration: It provides a comprehensive exploration of the hyperparameter space and guarantees that the optimal \n",
    "                                combination will be found if it exists within the specified grid.\n",
    "            5.Suitable for Smaller Grids: GridSearchCV is suitable when the hyperparameter space is relatively small, and computational \n",
    "                                           resources are sufficient to perform an exhaustive search.\n",
    "                \n",
    "    Randomized Search Cross-Validation (RandomizedSearchCV):\n",
    "\n",
    "        1. Search Strategy: RandomizedSearchCV samples a fixed number of combinations from the hyperparameter space randomly.\n",
    "        2. Exploration Method: It focuses on exploring a random subset of the hyperparameter space, potentially saving computational\n",
    "                                resources compared to GridSearchCV.\n",
    "        3. Computational Efficiency: RandomizedSearchCV is computationally more efficient than GridSearchCV since it doesn't\n",
    "                                     explore the entire space. It's particularly useful when the hyperparameter space is vast.\n",
    "        4. Less Comprehensive: It might not guarantee that the optimal combination will be found, but it increases the chance of finding\n",
    "                                good hyperparameters within the sampled space.\n",
    "        5. Suitable for Larger Grids: RandomizedSearchCV is more suitable when the hyperparameter space is large, and an exhaustive \n",
    "                                      search using GridSearchCV would be impractical due to time and resources.\n",
    "            \n",
    "    Choosing Between GridSearchCV and RandomizedSearchCV:\n",
    "\n",
    "         GridSearchCV: Choose GridSearchCV when you have a relatively small hyperparameter space and computational resources are sufficient\n",
    "                       for an exhaustive search. This approach is suitable when you want to ensure a thorough exploration of the entire grid \n",
    "                       and are willing to spend more time on tuning.\n",
    "\n",
    "        RandomizedSearchCV: Choose RandomizedSearchCV when you have a large hyperparameter space or limited computational resources. \n",
    "                            It's more time-efficient and can lead to good results by exploring a random subset of the space. While it\n",
    "                            might not guarantee the absolute best hyperparameters, it often finds very competitive solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a9a14-503f-47ad-ab20-3306cbf0e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425ee6b-f9fb-4413-8cc9-49d30ed7f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Data leakage refers to the situation where information from the test or validation data unintentionally leaks into the training \n",
    "      data during the model-building process. This can lead to overly optimistic or unrealistic performance estimates, as the model is \n",
    "      inadvertently learning patterns that it should not have access to. Data leakage can result in a model that appears to perform \n",
    "      well during evaluation but fails to generalize to new, unseen data.\n",
    "    \n",
    "    Why Data Leakage is a Problem:\n",
    "\n",
    "            Data leakage undermines the integrity of the model evaluation process and can have serious consequences for real-world deployment:\n",
    "\n",
    "            1. Overestimated Performance: Models trained on leaked data might perform exceptionally well on validation or test data but poorly \n",
    "                                          on new data because they have learned patterns specific to the validation/test set.\n",
    "\n",
    "            2. Misleading Decision-Making: Models with artificially high performance estimates can lead to incorrect business decisions or \n",
    "                                           ineffective interventions.\n",
    "\n",
    "            3.Wasted Resources: Investing resources based on inflated model performance can result in projects that underdeliver or fail to\n",
    "                                meet expectations.\n",
    "\n",
    "            4. Ethical Concerns: Using leaked information for decision-making might raise ethical concerns, especially if sensitive or \n",
    "                                 confidential data is involved.\n",
    "                \n",
    "        Example of Data Leakage:\n",
    "\n",
    "                Suppose you're building a credit card fraud detection model. You have a dataset with information about credit card \n",
    "                transactions, including transaction amounts, locations, timestamps, and whether a transaction was fraudulent or not.\n",
    "                \n",
    "            Data Leakage Scenario:\n",
    "\n",
    "                    1. While preprocessing the data, you accidentally include the transaction timestamp as a feature in the model.\n",
    "                    2. You split the data into a training set and a validation set, ensuring that the training data contains transactions \n",
    "                        that occurred before the validation data.\n",
    "                    3. During feature engineering, you create a new feature that calculates the average transaction amount for each user,\n",
    "                       using transactions from both the training and validation data.\n",
    "                    4. You train a machine learning model using the combined feature set, including the transaction timestamp and the\n",
    "                       average transaction amount.\n",
    "                    5. model achieves very high accuracy on the validation data, but when deployed, it fails to perform well on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb1159-7ee2-4af9-a234-4c3824e95bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29154557-0b11-4124-a534-ba08f02a64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Preventing data leakage is crucial to ensure that your machine learning model's performance estimates are realistic and that\n",
    "      the model generalizes well to new, unseen data. Here are some strategies to prevent data leakage during model building:\n",
    "        \n",
    "        1. Data Splitting Before Any Preprocessing:\n",
    "                Always split your dataset into training, validation, and test sets before any data preprocessing or feature engineering \n",
    "                steps. This ensures that information from the validation or test set doesn't accidentally influence the preprocessing choices.\n",
    "\n",
    "        2. Feature Selection and Engineering:\n",
    "                When creating features, ensure that the information you use to create them is only from the training set. Avoid using \n",
    "                information that comes from the validation or test sets. Features should be based solely on the training data distribution.\n",
    "                \n",
    "        3.Temporal Data and Time Series:\n",
    "                If you're working with temporal data, respect the chronological order when splitting the data. The training data should\n",
    "                precede the validation and test data to prevent the model from learning future information.\n",
    "        \n",
    "        4.Cross-Validation:\n",
    "                Use cross-validation techniques like k-fold cross-validation to evaluate your model's performance. This helps ensure \n",
    "                that the model's evaluation is consistent across different subsets of the data, preventing overfitting to a single validation set.\n",
    "\n",
    "        5. Regularization and Hyperparameter Tuning:\n",
    "                Perform hyperparameter tuning and regularization using only the training data. Avoid using validation or test data \n",
    "                for model selection or tuning.\n",
    "\n",
    "        6. Pipelines:\n",
    "                Utilize machine learning pipelines to encapsulate your preprocessing and modeling steps. Pipelines can help ensure that \n",
    "                preprocessing is performed consistently on both training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67eae3b-26bd-4a7c-86e2-ce87600b0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92597414-62ea-48d4-8c8a-35c5936be2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : A confusion matrix is a tabular representation that summarizes the performance of a classification model on a set of data by \n",
    "      comparing predicted class labels with the actual class labels. It provides insights into how well a model is classifying instances\n",
    "      into different classes and allows you to assess the model's performance in terms of true positives, true negatives, false positives,\n",
    "      and false negatives.\n",
    "    \n",
    "    A confusion matrix is typically organized as follows:\n",
    "        \n",
    "\n",
    "                                    Predicted Positive\t   Predicted Negative\n",
    "            \n",
    "                Actual Positive\t    True Positive (TP)\t   False Negative (FN)\n",
    "                \n",
    "                Actual Negative\t    False Positive (FP)\t   True Negative (TN)\n",
    "                \n",
    "    Terms in the Confusion Matrix:\n",
    "\n",
    "            True Positive (TP):  Instances that are actually positive and were correctly predicted as positive by the model.\n",
    "            False Positive (FP): Instances that are actually negative but were incorrectly predicted as positive by the model.\n",
    "            True Negative (TN):  Instances that are actually negative and were correctly predicted as negative by the model.\n",
    "            False Negative (FN): Instances that are actually positive but were incorrectly predicted as negative by the model.\n",
    "            \n",
    "            Interpretation of the Confusion Matrix:\n",
    "\n",
    "        Accuracy: Overall correctness of the model's predictions.\n",
    "            \n",
    "            Accuracy= (TP+TN)/TP+TN+FP+FN\n",
    "\n",
    "        Precision: Proportion of instances predicted as positive that are actually positive.\n",
    "  \n",
    "         Precision= TP/(TP+FP)\n",
    "\n",
    "        Recall (Sensitivity or True Positive Rate): Proportion of actual positive instances that were correctly predicted as positive.\n",
    "\n",
    "         Recall= TP/(TP+FN)\n",
    "\n",
    "        Specificity (True Negative Rate): Proportion of actual negative instances that were correctly predicted as negative.\n",
    "\n",
    "        Specificity= TN/(TN+FP)\n",
    "\n",
    "        F1-Score: Harmonic mean of precision and recall. It balances precision and recall and is useful when class distribution is imbalanced.\n",
    "\n",
    "                F1-Score = (2 × Precision × Recall) / Precision + Recall\n",
    "        \n",
    "        Confusion matrices are especially useful when evaluating the performance of a classification model, particularly in scenarios where \n",
    "        the class distribution is imbalanced. They provide a clear breakdown of the types of errors the model is making, helping to identify \n",
    "        areas where the model might need improvement. By analyzing the confusion matrix, you can make informed decisions about model adjustments, \n",
    "        feature engineering, or hyperparameter tuning to optimize the model's performance for the specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea97b3-ba9b-45b3-ba4e-23743f942db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0578d5a-0595-4e49-99ee-1fbd629c5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Precision:\n",
    "            Precision, also known as positive predictive value, measures the proportion of instances that the model predicted\n",
    "            as positive (true positives) that are actually positive. In other words, it answers the question: \"Of all instances predicted\n",
    "            as positive, how many were correctly predicted?\"\n",
    "            \n",
    "         Precision= TP/(TP+FP)\n",
    "        \n",
    "        High precision indicates that the model's positive predictions are likely to be correct. A high precision is desirable when false\n",
    "        positives (incorrectly labeled as positive) are costly or undesirable, such as in medical diagnoses where false positives lead to\n",
    "        unnecessary treatments.\n",
    "    \n",
    "    Recall (Sensitivity or True Positive Rate):\n",
    "        Recall, also known as sensitivity or the true positive rate, measures the proportion of actual positive instances that the model\n",
    "        correctly predicted as positive. It answers the question: \"Of all actual positive instances, how many were correctly predicted?\"\n",
    "        \n",
    "          Recall= TP/(TP+FN)\n",
    "        \n",
    "        High recall indicates that the model is effectively capturing most of the positive instances in the dataset. A high recall is\n",
    "        important when false negatives (actual positives incorrectly labeled as negative) are costly or when it's important to identify\n",
    "        as many true positive cases as possible, even if it means a higher rate of false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff37fcb-32ab-4b0e-83f3-35c0e2c3b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5cabb-229b-4ca5-a8fb-7177bb836401",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans :  A confusion matrix is a tabular representation that summarizes the performance of a classification model on a set of data by \n",
    "      comparing predicted class labels with the actual class labels. It provides insights into how well a model is classifying instances\n",
    "      into different classes and allows you to assess the model's performance in terms of true positives, true negatives, false positives,\n",
    "      and false negatives.\n",
    "    \n",
    "    A confusion matrix is typically organized as follows:\n",
    "        \n",
    "\n",
    "                                    Predicted Positive\t   Predicted Negative\n",
    "            \n",
    "                Actual Positive\t    True Positive (TP)\t   False Negative (FN)\n",
    "                \n",
    "                Actual Negative\t    False Positive (FP)\t   True Negative (TN)\n",
    "                \n",
    "    Terms in the Confusion Matrix:\n",
    "\n",
    "            True Positive (TP):  Instances that are actually positive and were correctly predicted as positive by the model.\n",
    "            False Positive (FP): Instances that are actually negative but were incorrectly predicted as positive by the model.\n",
    "            True Negative (TN):  Instances that are actually negative and were correctly predicted as negative by the model.\n",
    "            False Negative (FN): Instances that are actually positive but were incorrectly predicted as negative by the model.\n",
    "            \n",
    "            Interpretation of the Confusion Matrix:\n",
    "\n",
    "        Accuracy: Overall correctness of the model's predictions.\n",
    "            \n",
    "            Accuracy= (TP+TN)/TP+TN+FP+FN\n",
    "        \n",
    "        Precision: Proportion of instances predicted as positive that are actually positive.\n",
    "  \n",
    "         Precision= TP/(TP+FP)\n",
    "\n",
    "        Recall (Sensitivity or True Positive Rate): Proportion of actual positive instances that were correctly predicted as positive.\n",
    "\n",
    "         Recall= TP/(TP+FN)\n",
    "\n",
    "        Specificity (True Negative Rate): Proportion of actual negative instances that were correctly predicted as negative.\n",
    "\n",
    "        Specificity= TN/(TN+FP)\n",
    "\n",
    "        F1-Score: Harmonic mean of precision and recall. It balances precision and recall and is useful when class distribution is imbalanced.\n",
    "\n",
    "                F1-Score = (2 × Precision × Recall) / Precision + Recall\n",
    "        \n",
    "        False Positive Rate: False Positive Rate FP / (FP+TN) measures the rate of false alarms among actual negatives.\n",
    "        \n",
    "        False Negative Rate: False Negative Rate FN / (FN+TP) measures the rate of missed positive instances among actual positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74bcdc-a36c-4324-8388-64bd4d2eb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc57d8-f8c0-4e2c-91af-b4d9e7e3c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Several common metrics can be derived from a confusion matrix to evaluate the performance of a classification model. \n",
    "      These metrics provide insights into the model's accuracy, precision, recall, F1-score, and more. Here are some common\n",
    "      metrics and their calculations based on the confusion matrix:\n",
    "            \n",
    "                                 Predicted Positive\t   Predicted Negative\n",
    "            \n",
    "                Actual Positive\t    True Positive (TP)\t   False Negative (FN)\n",
    "                \n",
    "                Actual Negative\t    False Positive (FP)\t   True Negative (TN)\n",
    "                \n",
    "    Metrics Derived from the Confusion Matrix:\n",
    "        \n",
    "        Accuracy: Overall correctness of the model's predictions.\n",
    "            \n",
    "            Accuracy= (TP+TN)/TP+TN+FP+FN\n",
    "        \n",
    "        Precision: Proportion of instances predicted as positive that are actually positive.\n",
    "  \n",
    "         Precision= TP/(TP+FP)\n",
    "\n",
    "        Recall (Sensitivity or True Positive Rate): Proportion of actual positive instances that were correctly predicted as positive.\n",
    "\n",
    "         Recall= TP/(TP+FN)\n",
    "\n",
    "        Specificity (True Negative Rate): Proportion of actual negative instances that were correctly predicted as negative.\n",
    "\n",
    "        Specificity= TN/(TN+FP)\n",
    "\n",
    "        F1-Score: Harmonic mean of precision and recall. It balances precision and recall and is useful when class distribution is imbalanced.\n",
    "\n",
    "                F1-Score = (2 × Precision × Recall) / Precision + Recall\n",
    "        \n",
    "        False Positive Rate: False Positive Rate FP / (FP+TN) measures the rate of false alarms among actual negatives.\n",
    "        \n",
    "        False Negative Rate: False Negative Rate FN / (FN+TP) measures the rate of missed positive instances among actual positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d2b9f5-f465-44a3-aabe-b9abac77390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c523279-f0e7-4e23-be5e-41681822fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans :  The accuracy of a model is one of the metrics derived from its confusion matrix, specifically the ratio of correct predictions \n",
    "        to the total number of predictions\n",
    "    \n",
    "    Accuracy: Overall correctness of the model's predictions.\n",
    "            \n",
    "            Accuracy= (TP+TN)/TP+TN+FP+FN\n",
    "            \n",
    "    1. High Accuracy:\n",
    "        High accuracy indicates that the model is making a significant number of correct predictions. However, a high accuracy might be\n",
    "        misleading if the dataset is imbalanced or if certain types of errors are more critical than others.\n",
    "\n",
    "    2. Balanced Classes:\n",
    "        In cases where classes are balanced (similar number of positive and negative instances), a high accuracy suggests that the \n",
    "        model is performing well on both classes.\n",
    "    3.Imbalanced Classes:\n",
    "        In cases of class imbalance (one class significantly larger than the other), accuracy can be misleading. \n",
    "        A model that predicts the majority class for every instance can achieve high accuracy, but it might perform poorly on the minority class.\n",
    "\n",
    "    4.Importance of Other Metrics:\n",
    "        Accuracy doesn't consider false positives and false negatives separately, which might be critical in some applications. \n",
    "        For example, in medical diagnoses, false negatives (missing actual positives) can have severe consequences, even if accuracy is high.\n",
    "\n",
    "    5. Accuracy's Limitations:\n",
    "        Accuracy can give a false sense of model performance, especially when evaluating imbalanced datasets. It's important to consider \n",
    "        additional metrics like precision, recall, F1-score, and ROC-AUC to get a more comprehensive understanding of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafed844-33a1-4c18-af17-309d584b7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1241b9-0c2b-4b84-8546-7be45b94f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : A confusion matrix can be a valuable tool for identifying potential biases or limitations in your machine learning model, \n",
    "      particularly when it comes to understanding how the model performs across different classes or groups within the data.\n",
    "    \n",
    "    1.Class Imbalance:\n",
    "        Look at the distribution of actual classes in the confusion matrix. If there is a significant class imbalance, the model \n",
    "        might be biased toward the majority class. This can lead to high accuracy while performing poorly on the minority class.\n",
    "\n",
    "    2. False Positives and False Negatives:\n",
    "        Examine the false positive and false negative values. These errors can provide insights into which class the model is \n",
    "        more likely to misclassify. Biases can arise if the model disproportionately misclassifies one group more than the other.\n",
    "    \n",
    "    3.Misclassification Patterns:\n",
    "        Analyze whether the model is consistently misclassifying specific groups or types of instances. If certain groups are \n",
    "        consistently misclassified, it could indicate bias or limitations in the training data.\n",
    "\n",
    "    4.Evaluation Metrics for Different Classes:\n",
    "        Compare precision, recall, F1-score, and other metrics across different classes. Differences in these metrics might indicate \n",
    "        that the model's performance is unevenly distributed among classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
