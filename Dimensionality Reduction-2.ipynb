{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91526a57-e513-4f4e-b6a5-d303437052d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba3639-23fc-469c-bf21-3d7f4f3ef767",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : In the context of Principal Component Analysis (PCA), a projection refers to the transformation of\n",
    "      data from its original high-dimensional space to a lower-dimensional space. PCA is a dimensionality \n",
    "      reduction technique used to simplify the complexity of high-dimensional data while preserving most of\n",
    "      its important characteristics\n",
    "\n",
    "    Here's how a projection is used in PCA:\n",
    "\n",
    "        1. Compute the Covariance Matrix: First, PCA computes the covariance matrix of the original data. \n",
    "           This matrix represents the relationships between different features (variables) in the dataset.\n",
    "        \n",
    "        2. Eigenvalue Decomposition: Next, PCA performs eigenvalue decomposition on the covariance matrix to\n",
    "           obtain its eigenvectors and eigenvalues. Eigenvectors represent the directions of maximum variance \n",
    "           in the data, and eigenvalues represent the magnitude of variance along those directions.\n",
    "        \n",
    "        3.Select Principal Components: PCA then selects a subset of eigenvectors, called principal components, \n",
    "          based on their corresponding eigenvalues. These principal components capture the most variance in the data.\n",
    "\n",
    "        4. Projection: Finally, PCA projects the original data onto the subspace spanned by the selected principal \n",
    "           components. This projection effectively transforms the data from its original high-dimensional space to \n",
    "           a lower-dimensional space defined by the principal components.\n",
    "        \n",
    "    The projected data in the lower-dimensional space retains most of the important information present in the \n",
    "    original data while reducing its dimensionality. This reduction facilitates easier visualization, analysis, \n",
    "    and often improves the performance of machine learning algorithms by reducing the computational complexity \n",
    "    and removing noise or irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489051c-c054-4342-b2fc-dc839502376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf51513-4278-48ae-9141-dd7e00dd4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : The optimization problem in Principal Component Analysis (PCA) aims to find the directions in the feature\n",
    "       space (eigenvectors) along which the data exhibits the maximum variance. This is typically achieved by \n",
    "       solving the eigenvalue decomposition of the covariance matrix of the data. Let's break down how the \n",
    "        optimization problem works and what it seeks to achieve:\n",
    "\n",
    "     1. Covariance Matrix Calculation: PCA starts by calculating the covariance matrix of the original data. \n",
    "        The covariance matrix represents the relationships between different features (variables) in the dataset.\n",
    "        It quantifies how much two variables change together.\n",
    "\n",
    "     2. Eigenvalue Decomposition: Next, PCA performs eigenvalue decomposition on the covariance matrix. This \n",
    "        decomposition yields a set of eigenvectors and eigenvalues. The eigenvectors represent the directions (or axes)\n",
    "        in the feature space, and the eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "        \n",
    "     3. Selection of Principal Components: The optimization problem in PCA involves selecting a subset of eigenvectors, \n",
    "        called principal components, based on their corresponding eigenvalues. These principal components capture the\n",
    "        most variance in the data.\n",
    "\n",
    "     4. Objective Function: The optimization problem in PCA can be framed as maximizing the variance of the projected \n",
    "        data along the selected principal components. Mathematically, this can be expressed as maximizing the trace of \n",
    "        the covariance matrix of the projected data, which is equivalent to maximizing the sum of the eigenvalues \n",
    "        associated with the selected principal components.\n",
    "        \n",
    "     5. Constraint: There is typically a constraint on the number of principal components to select, often determined \n",
    "        by the desired dimensionality reduction or the amount of variance explained. PCA allows for selecting fewer\n",
    "        principal components than the original dimensions of the data, effectively reducing its dimensionality.\n",
    "\n",
    "     6. Solution: The solution to the optimization problem involves finding the eigenvectors corresponding to the \n",
    "        largest eigenvalues, as these represent the directions of maximum variance in the data. These eigenvectors \n",
    "        form the basis for the lower-dimensional subspace onto which the data will be projected.\n",
    "        \n",
    "  the optimization problem in PCA seeks to find a lower-dimensional subspace that captures the maximum variance in \n",
    "  the data, facilitating dimensionality reduction while retaining as much relevant information as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0099cd32-1b14-4750-84a3-fb3be2fafb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7725440-43fe-441d-8ac0-772c012b1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : The relationship between covariance matrices and Principal Component Analysis (PCA) is fundamental to \n",
    "      understanding how PCA works.\n",
    "\n",
    " 1. Covariance Matrix: The covariance matrix is a square matrix that summarizes the pairwise covariances between\n",
    "    different features (variables) in a dataset. Given a dataset with \n",
    "            n observations and m features, the covariance matrix Σ is an m×m matrix defined as:\n",
    "            \n",
    "                    Σ= 1/n* (X−μ)^T(X−μ)\n",
    "                \n",
    "        where X is an n×m matrix containing the data points (each row represents an observation and each column \n",
    "        represents a feature), and μ is the mean vector of size m×1.\n",
    "        \n",
    " 2. PCA and Covariance Matrix: PCA utilizes the covariance matrix to identify the directions of maximum variance \n",
    "    in the dataset. The eigenvectors of the covariance matrix represent these directions, and the corresponding \n",
    "    eigenvalues represent the amount of variance explained along each eigenvector.\n",
    " \n",
    " 3. Eigenvalue Decomposition: PCA involves performing eigenvalue decomposition on the covariance matrix. This \n",
    "    decomposition yields a set of eigenvectors and eigenvalues. The eigenvectors form a new basis for the \n",
    "    feature space, and the eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    " \n",
    " 4. Projection: After obtaining the eigenvectors and eigenvalues, PCA projects the original data onto a \n",
    "    lower-dimensional subspace spanned by a subset of the eigenvectors, known as principal components. \n",
    "    This projection effectively transforms the data from its original high-dimensional space to a \n",
    "    lower-dimensional space while preserving most of the variance.\n",
    "\n",
    " 5. Dimensionality Reduction: PCA allows for dimensionality reduction by selecting a subset of principal \n",
    "    components that capture the most variance in the data. Typically, this selection is based on retaining \n",
    "    a certain percentage of the total variance or specifying the desired number of dimensions.\n",
    "    \n",
    " In summary, the covariance matrix plays a central role in PCA by providing information about the relationships \n",
    " between features in the dataset. PCA utilizes this information to identify the directions of maximum variance,\n",
    " which are represented by the eigenvectors of the covariance matrix. These eigenvectors form the basis for \n",
    " dimensionality reduction and data transformation in PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adaac7b-4517-4c60-a1d2-b5a0e426d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e0148-4824-4012-b605-1cdcc7a1fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : The choice of the number of principal components in PCA can significantly impact its performance and the\n",
    "      effectiveness of dimensionality reduction. Here's how:\n",
    "  \n",
    "    1. Dimensionality Reduction: PCA aims to reduce the dimensionality of the data while preserving most of its \n",
    "       important information. The number of principal components chosen determines the dimensionality of the reduced\n",
    "        space. Selecting fewer principal components leads to greater compression of the data but may result in some \n",
    "        loss of information.\n",
    "\n",
    "    2. Explained Variance: Each principal component captures a certain amount of variance in the data. The cumulative\n",
    "       explained variance of the principal components depends on the number selected. Choosing a larger number of principal \n",
    "        components results in capturing more of the total variance in the data. This is typically visualized using a scree \n",
    "        plot or cumulative explained variance plot.\n",
    "    \n",
    "    3. Information Retention: The choice of the number of principal components determines how much information is retained \n",
    "       from the original dataset. Higher numbers of principal components retain more information but may also retain \n",
    "        noise or irrelevant features. Conversely, selecting too few principal components may result in loss of important\n",
    "        information.\n",
    "\n",
    "    4. Computational Efficiency: Using fewer principal components reduces the computational complexity of PCA, making it \n",
    "       faster to compute and easier to interpret. This can be advantageous when dealing with large datasets or when \n",
    "        computational resources are limited.\n",
    "    \n",
    "    5. Overfitting vs. Underfitting: Similar to other dimensionality reduction techniques, PCA involves a trade-off \n",
    "       between overfitting and underfitting. Choosing too many principal components may lead to overfitting, where\n",
    "        the model captures noise or irrelevant features in the data. On the other hand, selecting too few principal \n",
    "        components may lead to underfitting, where important patterns or structures in the data are not captured adequately.\n",
    "\n",
    "    6. Application-Specific Considerations: The optimal number of principal components may vary depending on the \n",
    "       specific application or task. It is often determined empirically through cross-validation or other validation \n",
    "        techniques. Additionally, domain knowledge and understanding of the data can help in selecting an appropriate \n",
    "        number of principal components.\n",
    "    \n",
    " In summary, the choice of the number of principal components in PCA is a crucial decision that affects the balance\n",
    " between dimensionality reduction, information retention, computational efficiency, and the potential for overfitting\n",
    " or underfitting. It requires careful consideration and experimentation based on the characteristics of the dataset \n",
    " and the objectives of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae44dd7-9f3d-453e-b713-6fbde46c9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759c0cb-9d66-4368-b517-0b78caca52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : PCA can be used for feature selection indirectly by identifying the most important dimensions (principal components)\n",
    "      that capture the variance in the data. Here's how PCA can be utilized for feature selection and its benefits:\n",
    "\n",
    "    1. Identifying Important Features: PCA identifies the principal components that explain the most variance in the data.\n",
    "       Each principal component is a linear combination of the original features. By examining the loadings (coefficients)\n",
    "        of the original features in each principal component, one can identify which original features contribute the most\n",
    "        to the variance captured by those components.\n",
    "    \n",
    "    2. Dimensionality Reduction: PCA reduces the dimensionality of the data by projecting it onto a lower-dimensional subspace \n",
    "       spanned by the selected principal components. Features that contribute little to the variance in the data are likely \n",
    "        to have low loadings on the selected principal components and may be considered less important for modeling or analysis \n",
    "        purposes.\n",
    "\n",
    "    3. Noise Reduction: PCA can help in filtering out noise or irrelevant features by focusing on the principal components\n",
    "       that capture the most variance in the data. Features that contribute mainly to noise or random fluctuations are likely\n",
    "        to have low loadings on the selected principal components and may be effectively disregarded in subsequent analysis.\n",
    "    \n",
    "    4. Simplifying Model Interpretation: By selecting a subset of principal components that capture the most important variance\n",
    "       in the data, PCA simplifies the interpretation of models built on the reduced feature space. Instead of dealing with a \n",
    "        large number of original features, one can work with a smaller set of principal components, which often leads to simpler \n",
    "        and more interpretable models.\n",
    "\n",
    "    5. Addressing Multicollinearity: PCA can help address multicollinearity (high correlation between features) by transforming \n",
    "       the original features into uncorrelated principal components. This can be particularly useful in regression analysis, \n",
    "        where multicollinearity can lead to unstable estimates of coefficients.\n",
    "    \n",
    "    6. Improving Model Performance: By reducing the dimensionality of the feature space while retaining most of the important \n",
    "       information, PCA can lead to more efficient model training and improved generalization performance. Models trained on \n",
    "        the reduced feature space may be less prone to overfitting and may generalize better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a0c583-f6d9-4387-b100-217cb7460076",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f89dca-bccc-4dc3-be08-95e19a63c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Principal Component Analysis (PCA) finds a wide range of applications across various domains in data science and \n",
    "      machine learning. Some common applications include:\n",
    "    \n",
    "    1. Dimensionality Reduction: PCA is extensively used for reducing the dimensionality of datasets with a large number \n",
    "       of features. By projecting the data onto a lower-dimensional subspace spanned by the principal components, PCA \n",
    "        simplifies the dataset while preserving most of its important information. This is beneficial for visualization, \n",
    "        speeding up computations, and improving the performance of machine learning algorithms.\n",
    "\n",
    "    2. Feature Extraction: PCA can be used to extract a smaller set of features (principal components) that capture the most\n",
    "       important patterns or variations in the data. These extracted features can then be used as inputs for downstream machine\n",
    "        learning tasks, such as classification, regression, clustering, or anomaly detection.\n",
    "    \n",
    "    3. Data Visualization: PCA is widely employed for visualizing high-dimensional datasets in a lower-dimensional space \n",
    "       (usually 2D or 3D). By projecting the data onto a small number of principal components, PCA enables the visualization\n",
    "        of complex datasets in a more interpretable form, facilitating exploratory data analysis and insights discovery.\n",
    "\n",
    "    4. Image Compression: In image processing and computer vision, PCA can be utilized for compressing images by representing\n",
    "       them in terms of a smaller number of principal components. This reduces the storage space required for storing images \n",
    "        while preserving their essential features, making PCA an efficient technique for image compression and storage.\n",
    "    \n",
    "    5. Signal Processing: PCA finds applications in signal processing tasks such as denoising, feature extraction, and \n",
    "       dimensionality reduction. It can help in separating signal from noise by identifying the principal components that \n",
    "        capture the underlying signal structure while filtering out noise components.\n",
    "\n",
    "    6. Collaborative Filtering: In recommender systems, PCA can be used for collaborative filtering by reducing the \n",
    "       dimensionality of the user-item interaction matrix. This enables efficient recommendation of items to users based on \n",
    "        their preferences and similarity to other users or items.\n",
    "    \n",
    "    7. Genomics and Bioinformatics: PCA is applied in analyzing high-dimensional biological data such as gene expression\n",
    "       profiles, DNA sequences, or protein structures. It helps in identifying patterns, clustering similar samples, and \n",
    "        discovering relationships between biological variables.\n",
    "\n",
    "    8. Financial Modeling: PCA is used in financial modeling for risk management, portfolio optimization, and asset pricing. \n",
    "       It helps in identifying the underlying factors driving the covariance structure of financial assets and in constructing\n",
    "        diversified portfolios with reduced risk.\n",
    "    \n",
    " Overall, PCA is a versatile and widely used technique in data science and machine learning, offering solutions to various \n",
    "challenges related to dimensionality reduction, feature extraction, data visualization, and pattern discovery across diverse\n",
    "application domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7e7a8-5387-412e-8347-b46fd4bb5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7.What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006ebe8-f712-471e-8354-df56c23c6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans: In Principal Component Analysis (PCA), the spread and variance are closely related concepts, as they both \n",
    "     relate to the dispersion or variability of data points along different dimensions. Here's how they are related:\n",
    "     \n",
    "    1. Spread: In the context of PCA, spread refers to the extent or range of variation in the data along different \n",
    "       axes or dimensions. It indicates how widely the data points are distributed in the feature space.\n",
    "\n",
    "    2. Variance: Variance, on the other hand, quantifies the amount of dispersion or variability of data points around\n",
    "       the mean along a particular dimension. It measures the average squared deviation of data points from the mean \n",
    "        along a specific axis or direction.\n",
    "    \n",
    "    The relationship between spread and variance in PCA can be understood through the eigenvalues of the covariance \n",
    "    matrix. When performing PCA, one of the primary goals is to find the directions (principal components) along which \n",
    "    the data exhibits the maximum spread or variance. These directions are determined by the eigenvectors of the covariance\n",
    "    matrix, while the corresponding eigenvalues represent the amount of variance explained along each principal component.\n",
    "    \n",
    "    Specifically:\n",
    "\n",
    "        - Eigenvectors: The eigenvectors of the covariance matrix represent the directions in the feature space along \n",
    "          which the data exhibits maximum spread. Each eigenvector corresponds to a principal component, and together \n",
    "            they form a new basis for the feature space.\n",
    "        \n",
    "        - Eigenvalues: The eigenvalues associated with the eigenvectors quantify the amount of variance explained along \n",
    "          each principal component. Larger eigenvalues indicate that the corresponding principal components capture more \n",
    "            variance in the data, while smaller eigenvalues indicate less variance.\n",
    "        \n",
    "    In summary, in PCA, spread and variance are related through the eigenvalues of the covariance matrix. The directions \n",
    "    of maximum spread (principal components) are determined by the eigenvectors, while the eigenvalues quantify the amount\n",
    "    of variance explained along each principal component. By selecting the principal components with the largest eigenvalues, \n",
    "    PCA captures the most important sources of variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e115b69-2fd7-40fd-afbf-2075f81a8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f1259-12ac-4ea6-8954-817f3c7e3658",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans: Principal Component Analysis (PCA) utilizes the spread and variance of the data to identify the principal components, \n",
    "     which are the directions in the feature space that capture the maximum variance. Here's how PCA uses spread and variance \n",
    "     to identify principal components:\n",
    "\n",
    "  1. Covariance Matrix: PCA starts by computing the covariance matrix of the original data. The covariance matrix summarizes\n",
    "     the relationships between different features (variables) in the dataset and quantifies how much two variables change\n",
    "      together. It provides information about the spread and correlation structure of the data.\n",
    "\n",
    "  2. Eigenvalue Decomposition: Next, PCA performs eigenvalue decomposition on the covariance matrix. This decomposition \n",
    "     yields a set of eigenvectors and eigenvalues. Eigenvectors represent the directions in the feature space, and eigenvalues\n",
    "     represent the amount of variance explained along those directions.\n",
    "\n",
    "  3. Principal Components: The eigenvectors of the covariance matrix represent the principal components of the data. These are\n",
    "     the directions of maximum spread or variance in the dataset. The eigenvector corresponding to the largest eigenvalue \n",
    "     represents the direction along which the data exhibits the most variability, followed by subsequent eigenvectors in \n",
    "        decreasing order of eigenvalues.\n",
    "\n",
    "  4. Selection of Principal Components: PCA selects a subset of the eigenvectors, known as principal components, based on\n",
    "     their corresponding eigenvalues. Typically, the principal components associated with the largest eigenvalues are \n",
    "      retained, as they capture the most variance in the data. The number of principal components selected is often \n",
    "        determined by the desired level of dimensionality reduction or the amount of variance explained.\n",
    " \n",
    "  5. Projection: Finally, PCA projects the original data onto the subspace spanned by the selected principal components. \n",
    "     This projection transforms the data from its original high-dimensional space to a lower-dimensional space while\n",
    "        preserving most of the variance. Each data point is represented by its coordinates in the new feature space \n",
    "        defined by the principal components.\n",
    "\n",
    " In summary, PCA identifies principal components by leveraging the spread and variance of the data, as quantified \n",
    " by the covariance matrix and its eigenvalues. By selecting the directions of maximum variance, PCA captures the most\n",
    "    important sources of variability in the data, facilitating dimensionality reduction and data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72c449-9ed7-4dd5-a2ff-c056a5899d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa4ab5-61ef-44e1-8d0d-249268028368",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Principal Component Analysis (PCA) handles data with high variance in some dimensions but low variance in others by \n",
    "      identifying the directions (principal components) of maximum variance and focusing on those dimensions for dimensionality\n",
    "      reduction. Here's how PCA addresses this scenario:\n",
    "\n",
    " 1. Identifying Principal Components: PCA identifies the principal components by performing eigenvalue decomposition on the\n",
    "    covariance matrix of the data. The eigenvectors of the covariance matrix represent the directions in the feature space \n",
    "    that capture the maximum variance. These directions are sorted by the corresponding eigenvalues, with the highest \n",
    "    eigenvalues indicating the principal components that explain the most variance in the data.\n",
    "\n",
    " 2. Dimensionality Reduction: PCA selects a subset of the principal components based on their corresponding eigenvalues.\n",
    "    Typically, the principal components associated with the largest eigenvalues are retained, as they capture the most \n",
    "    variance in the data. By focusing on the dimensions with high variance, PCA effectively reduces the dimensionality of \n",
    "    the dataset while preserving most of its important information.\n",
    "\n",
    " 3. Projection: After selecting the principal components, PCA projects the original data onto the subspace spanned by these \n",
    "    components. This projection transforms the data from its original high-dimensional space to a lower-dimensional space \n",
    "    defined by the principal components. Data points are represented by their coordinates in this new feature space.\n",
    "\n",
    " 4. Dimensionality Adjustment: PCA implicitly adjusts for the differences in variance across dimensions by prioritizing the\n",
    "    dimensions with higher variance during the dimensionality reduction process. Dimensions with low variance contribute less\n",
    "    to the overall spread of the data and are therefore given less weight in determining the principal components.\n",
    "\n",
    " 5. Data Transformation: Through dimensionality reduction and data transformation, PCA effectively captures the most \n",
    "    significant sources of variability in the data while discarding or compressing dimensions with low variance. This \n",
    "    allows for a more compact representation of the data that retains most of its important characteristics.\n",
    "    \n",
    "    In summary, PCA handles data with high variance in some dimensions but low variance in others by identifying the\n",
    "    principal components that capture the maximum variance and focusing on those dimensions for dimensionality reduction. \n",
    "    By prioritizing dimensions with high variance, PCA effectively reduces the dimensionality of the dataset while preserving\n",
    "    most of its important information.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
