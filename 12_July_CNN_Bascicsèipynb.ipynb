{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "165b1cae-edc4-4de4-af11-7bd20532e4cc",
   "metadata": {},
   "source": [
    "#  1 - Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6037134-f8d7-427c-9716-81e97415250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Describe the purpose and benefits of pooling in CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf21b1-e05e-4b5b-8020-ae17d3e95b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Purpose of Pooling in CNNs:\n",
    "    \n",
    "        1 Spatial Hierarchical Representation:\n",
    "                Pooling helps create a spatial hierarchy in the network by progressively reducing \n",
    "                the spatial dimensions of the feature maps. This allows the network to focus on \n",
    "                more abstract and higher-level features in deeper layers.\n",
    "            \n",
    "        2. Dimensionality Reduction:\n",
    "                Pooling reduces the spatial dimensions of the input feature maps, which leads to\n",
    "                a decrease in the number of parameters in the network. This reduction in dimensionality\n",
    "                is essential for managing computational resources and preventing overfitting.\n",
    "                \n",
    "        3. Translation Invariance:\n",
    "                Pooling introduces a degree of translation invariance by selecting the most important \n",
    "                features in a local neighborhood. This is particularly useful in image recognition tasks\n",
    "                where the position of a feature in the input shouldn't drastically affect its recognition.\n",
    "        \n",
    "        4. Feature Invariance:\n",
    "                Pooling helps in creating feature maps that are more invariant to small changes in the\n",
    "                input. By selecting the maximum or average value within a local region, pooling captures\n",
    "                the presence of a feature while being less sensitive to its precise location.\n",
    "                \n",
    "    Benefits of Pooling in CNNs:\n",
    "        1. Computational Efficiency:\n",
    "                Pooling reduces the spatial dimensions of the feature maps, leading to a decrease in \n",
    "                the number of computations in subsequent layers. This is crucial for efficiency in both \n",
    "                training and inference, especially when dealing with large datasets and complex networks.\n",
    "                \n",
    "        2. Parameter Reduction:\n",
    "                By reducing the spatial dimensions, pooling helps in reducing the number of parameters in \n",
    "                the network. Fewer parameters make the model less prone to overfitting and more manageable \n",
    "                in terms of memory and computation.\n",
    "                \n",
    "        3. Robustness and Generalization:\n",
    "                Pooling contributes to the robustness of the model by emphasizing essential features and\n",
    "                discarding less relevant information. This aids in generalizing well to unseen data and \n",
    "                prevents the model from memorizing noise in the training set.\n",
    "                \n",
    "        4. Hierarchical Feature Learning:\n",
    "                Through the progressive reduction of spatial dimensions, pooling facilitates hierarchical \n",
    "                feature learning. Deeper layers in the network capture increasingly abstract and complex \n",
    "                features, enabling the network to learn hierarchical representations of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794a9b7-12ef-4118-919a-23841926c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain the difference between min pooling and max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdca158-956b-44fa-9c64-9f5be96d75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Min pooling and max pooling are two types of pooling operations used in Convolutional Neural \n",
    "      Networks (CNNs) for down-sampling feature maps. Both operations aim to reduce the spatial \n",
    "      dimensions of the input while retaining important information. The key difference lies in how\n",
    "      they select values from the local regions.\n",
    "    \n",
    "    1. Max Pooling:\n",
    "\n",
    "            a.) Operation: In max pooling, the maximum value within a local region (usually a 2x2 or \n",
    "                3x3 window) is selected and retained as the output value for that region.\n",
    "            b.) Purpose: Max pooling is designed to capture the most prominent feature in a given \n",
    "                local region. It emphasizes the presence of specific features while suppressing less\n",
    "                relevant information. Max pooling is effective for detecting the most activated features\n",
    "                in different parts of an image.\n",
    "    \n",
    "    2.Min Pooling:\n",
    "\n",
    "            a>) Operation: In min pooling, the minimum value within a local region is selected and \n",
    "                retained as the output value for that region.\n",
    "            b.) Purpose: Min pooling, on the other hand, emphasizes the least value in a local region. \n",
    "                This can be useful in scenarios where the presence of a specific feature is indicated by \n",
    "                the minimum value in that region. However, min pooling is less common than max pooling in practice.\n",
    "                \n",
    "    In both max pooling and min pooling, the pooling operation is applied independently to each local region across\n",
    "    the entire input feature map. These operations contribute to down-sampling the spatial dimensions, making the \n",
    "    subsequent layers of the neural network more computationally efficient and reducing the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d8365-6172-4e1a-bd3f-a684ad49ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Discuss the concept of padding in CNN and its significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909253b0-063d-4bc9-ae45-edc83a94b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Padding in Convolutional Neural Networks (CNNs) refers to the addition of extra pixels or values around\n",
    "      the input data before applying the convolution operation. It involves extending the boundaries of the \n",
    "      input feature map with zeros or other constant values. Padding is a crucial concept in CNNs, and its\n",
    "      significance lies in addressing several issues associated with the convolutional layers.\n",
    "    \n",
    "    Significance of Padding in CNNs:\n",
    "        1. Preserving Spatial Information:\n",
    "                Without padding, the spatial dimensions of the feature map reduce after each convolutional\n",
    "                layer. Padding helps maintain the spatial information at the borders of the input, ensuring\n",
    "                that the convolutional operation considers information from the entire input image.\n",
    "\n",
    "        2. Avoiding Information Loss at Borders:\n",
    "                During convolution, the filters are typically applied to the central part of the input, \n",
    "                and the border pixels receive less attention. Padding helps prevent information loss at\n",
    "                the borders by ensuring that all pixels have an equal opportunity to be part of the \n",
    "                convolution operation.\n",
    "        \n",
    "        3. Centering the Convolutional Operation:\n",
    "                Padding allows the filter or kernel to be centered over each pixel in the input feature\n",
    "                map. This is important for learning features at different locations within the input, \n",
    "                and it helps maintain symmetry in the network.\n",
    "\n",
    "        4. Controlling the Spatial Dimensions:\n",
    "                Padding enables control over the spatial dimensions of the feature maps. By adjusting the\n",
    "                amount of padding, one can influence the size of the output feature maps after convolution,\n",
    "                which is important for designing network architectures and managing computational complexity.\n",
    "    \n",
    "        5. Preventing Shrinking Feature Maps:\n",
    "                Without padding, as convolution is applied, the spatial dimensions of the feature maps \n",
    "                decrease. Padding helps prevent excessive reduction in size, which can be particularly \n",
    "                important when working with small input images or when constructing deeper networks.\n",
    "                \n",
    "    Types of Padding:\n",
    "        1.Zero Padding:\n",
    "            The most common type, where extra pixels are added with zero values around the input feature map.\n",
    "        2.Same Padding:\n",
    "            A specific type of padding where the input and output feature maps have the same spatial dimensions.\n",
    "            Achieved by determining the padding size that makes the output size equal to the input size.\n",
    "        3.Reflective Padding:\n",
    "            Padding by reflecting the values at the borders of the input feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca79c8-d0a3-4c1e-9acb-4821551605cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output\n",
    "    feature map size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0872d-cc68-4577-9745-489bbb6b4e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Zero-padding:\n",
    "        Definition:\n",
    "                In zero-padding, extra pixels with zero values are added around the input \n",
    "                feature map before applying the convolution operation.\n",
    "        Effect on Output Size:\n",
    "                Zero-padding increases the spatial dimensions of the input feature map. \n",
    "                For a convolutional layer with zero-padding, the output size (feature map)\n",
    "                will often be larger than the input size.\n",
    "        Preservation of Spatial Information:\n",
    "                Zero-padding helps preserve spatial information at the borders of the input\n",
    "                feature map by ensuring that all pixels have an equal opportunity to be part \n",
    "                of the convolutional operation.\n",
    "        Centering the Convolutional Operation:\n",
    "                Zero-padding allows the filter or kernel to be centered over each pixel in the \n",
    "                input feature map, which is important for learning features at different locations \n",
    "                within the input.\n",
    "                \n",
    "    Valid-padding:\n",
    "        Definition:\n",
    "                Valid-padding, also known as \"no-padding,\" refers to the absence of extra pixels around \n",
    "                the input feature map before applying the convolution operation.\n",
    "        Effect on Output Size:\n",
    "                Without padding, the spatial dimensions of the input feature map reduce after each \n",
    "                convolutional layer. Therefore, the output size (feature map) will be smaller than the input size.\n",
    "        Information Loss at Borders:\n",
    "                One drawback of valid-padding is that it may lead to information loss at the borders of\n",
    "                the input feature map. The pixels at the edges receive less attention during the convolution operation.\n",
    "        Shrinking Feature Maps:\n",
    "                Valid-padding can result in a significant reduction in the size of the output feature maps,\n",
    "                especially as convolutional layers are applied sequentially.\n",
    "                \n",
    "    Comparison:\n",
    "        Output Size:\n",
    "                -> Zero-padding generally results in larger output feature maps compared to valid-padding,\n",
    "                   as it extends the spatial dimensions.\n",
    "                -> Valid-padding leads to smaller output feature maps as it lacks the extra pixels around the input.\n",
    "        Preservation of Information:\n",
    "                -> Zero-padding helps in preserving information at the borders and provides a more symmetric \n",
    "                   treatment of pixels.\n",
    "                -> Valid-padding may cause information loss at the edges, as convolutional operations do not \n",
    "                   consider pixels outside the input feature map.\n",
    "        Use Cases:\n",
    "                -> Zero-padding is often used when maintaining spatial information and symmetry is crucial, \n",
    "                   and when controlling the spatial dimensions of the feature maps is desired.\n",
    "                -> Valid-padding may be preferred when reducing the spatial dimensions aggressively is acceptable, \n",
    "                   and computational efficiency is a priority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc09ce-86cd-4076-b7f1-04e7183818ab",
   "metadata": {},
   "source": [
    "# 2 - Exploring LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae15965-aa73-4d33-b4a9-14fc6afb2422",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Provide a brief overview of LeNet-5 architectuce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96bced3-902d-4413-ab8b-df1543f149fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : LeNet-5 is a pioneering convolutional neural network (CNN) architecture designed for handwritten \n",
    "      digit recognition, specifically for recognizing characters from the MNIST dataset. Proposed by \n",
    "      Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner in 1998, LeNet-5 played a significant\n",
    "      role in the development of deep learning and convolutional neural networks. Here's a brief \n",
    "      overview of the LeNet-5 architecture:\n",
    "\n",
    "        Architecture:\n",
    "            1.Input Layer:\n",
    "                LeNet-5 takes as input grayscale images of size 32x32 pixels.\n",
    "            2.Convolutional Layers:\n",
    "                -> The network consists of two convolutional layers, each followed by a subsampling\n",
    "                   (pooling) layer.\n",
    "                -> The first convolutional layer uses a 5x5 kernel with 6 filters.\n",
    "                -> The second convolutional layer uses a 5x5 kernel with 16 filters.\n",
    "            3.Subsampling (Pooling) Layers:\n",
    "                -> After each convolutional layer, a subsampling layer is applied for down-sampling.\n",
    "                -> Max-pooling with a 2x2 window is used in both subsampling layers.\n",
    "            4. Fully Connected Layers:\n",
    "                -> Following the convolutional and subsampling layers, there are three fully connected layers.\n",
    "                -> The first fully connected layer has 120 neurons.\n",
    "                -> The second fully connected layer has 84 neurons.\n",
    "                -> The output layer has 10 neurons corresponding to the 10 possible digit classes (0 through 9).\n",
    "            5. Activation Function:\n",
    "                -> Sigmoid activation functions are used throughout the network.\n",
    "            6. Flattening:\n",
    "                -> Between the convolutional/subsampling layers and the fully connected layers, the feature \n",
    "                   maps are flattened into a vector.\n",
    "            7. Softmax Output:\n",
    "                -> The final layer uses a softmax activation function to compute the probabilities for each \n",
    "                   digit class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71decb6b-01fe-407f-af26-57909787d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Describe the key components of LeNet-5 and their respective purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a846673-1d11-4513-9cb7-7b10900b77e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Here are the key components of LeNet-5 and their respective purposes:\n",
    "         1. Input Layer:\n",
    "            -> Purpose: The input layer accepts grayscale images of size 32x32 \n",
    "              pixels. It serves as the initial stage for processing input data.\n",
    "         2. Convolutional Layers:\n",
    "            -> Purpose: The convolutional layers are responsible for learning\n",
    "               local patterns and features from the input images. LeNet-5 has \n",
    "               two convolutional layers with 5x5 kernels.\n",
    "         3. Subsampling (Pooling) Layers:\n",
    "            -> Purpose: Subsampling layers, using max-pooling with a 2x2 window, follow \n",
    "               each convolutional layer. These layers down-sample the spatial dimensions \n",
    "               of the feature maps, reducing computational complexity and promoting translation invariance.\n",
    "        4. Fully Connected Layers:\n",
    "            -> Purpose: After the convolutional and subsampling layers, there are three fully \n",
    "               connected layers. These layers integrate high-level features learned by the convolutional\n",
    "                layers and make decisions about the presence of specific patterns. The first fully connected \n",
    "                layer has 120 neurons, the second has 84 neurons, and the output layer has 10 neurons for \n",
    "                digit classification.\n",
    "        5. Activation Function (Sigmoid):\n",
    "            -> Purpose: Sigmoid activation functions are used throughout the network. In the context of \n",
    "               LeNet-5, sigmoid activation introduces non-linearity to the model, allowing it to learn \n",
    "                complex mappings between the input data and output classes.\n",
    "        6. Flattening:\n",
    "            -> Purpose: After the convolutional and subsampling layers, the feature maps are flattened\n",
    "               into a vector before being fed into the fully connected layers. Flattening transforms the \n",
    "                spatial representation into a format suitable for dense layers.\n",
    "        7. Softmax Output:\n",
    "            -> Purpose: The final layer uses a softmax activation function, which converts the output\n",
    "               into probability distributions over the 10 possible digit classes (0 through 9). This allows \n",
    "                the network to make a probabilistic prediction about the input image.\n",
    "        8. Parameters (Weights and Biases):\n",
    "            -> Purpose: The learnable parameters, including weights and biases, are crucial for adapting \n",
    "               the model during training. These parameters are adjusted through backpropagation and gradient\n",
    "                descent to minimize the difference between predicted and actual outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985913df-c46d-4648-8019-8a3ea00ede89",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93073817-a979-4317-acec-ba4aa72ff287",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : Advantages of LeNet-5:\n",
    "        1.Pioneering Architecture:\n",
    "            -> LeNet-5 was one of the earliest successful CNN architectures, setting the foundation\n",
    "               for subsequent developments in deep learning and image classification.\n",
    "        2. Effective for Handwritten Digit Recognition:\n",
    "            -> LeNet-5 was specifically designed for handwritten digit recognition tasks, such as\n",
    "               those found in the MNIST dataset. It demonstrated high accuracy and effectiveness \n",
    "               for this specific application.\n",
    "        3.Spatial Hierarchical Features:\n",
    "            -> The use of convolutional and subsampling layers in LeNet-5 enables the learning of\n",
    "               spatial hierarchical features, allowing the network to capture local patterns and\n",
    "                gradually build up to more complex representations.\n",
    "        4. Translation Invariance:\n",
    "            -> The inclusion of subsampling layers contributes to translation invariance, making\n",
    "               the model less sensitive to slight shifts in the position of features within the input images.\n",
    "        5.Simple and Interpretable:\n",
    "            -> The architecture of LeNet-5 is relatively simple and interpretable. It consists of \n",
    "               convolutional layers, subsampling layers, and fully connected layers, making it easy\n",
    "                to understand and implement.\n",
    "        6.Parameter Efficiency:\n",
    "            -> LeNet-5 was designed with a relatively small number of parameters compared to modern\n",
    "               architectures, making it efficient in terms of memory usage and training time.\n",
    "                \n",
    "Limitations of LeNet-5:\n",
    "        \n",
    "        1. Limited Capacity for Complex Tasks:\n",
    "             -> LeNet-5 was designed for relatively simple tasks like digit recognition. Its\n",
    "                architecture may not have the capacity to handle more complex and diverse image\n",
    "                classification tasks seen in contemporary datasets.\n",
    "        2.Small Input Size:\n",
    "            -> The input size of 32x32 pixels limits its ability to capture fine-grained details in \n",
    "               larger images. Modern image classification tasks often involve higher-resolution images, \n",
    "                requiring larger and more complex architectures.\n",
    "        3.Sigmoid Activation Function:\n",
    "            -> The use of sigmoid activation functions in LeNet-5 may lead to challenges in training\n",
    "               deep networks. Modern architectures often use rectified linear units (ReLU) for faster convergence.\n",
    "        4.Pooling Operation:\n",
    "            -> Max-pooling, as used in LeNet-5, can lead to a loss of spatial information due to the \n",
    "               selection of only the maximum value in a local region. More recent architectures sometimes\n",
    "                use alternative pooling strategies to address this.\n",
    "        5. Lack of Batch Normalization and Regularization:\n",
    "            -> LeNet-5 does not incorporate modern techniques like batch normalization and advanced regularization\n",
    "                methods, which are important for stabilizing training and improving generalization.\n",
    "        6.Not Suitable for Large-Scale Datasets:\n",
    "            -> While effective for tasks like MNIST digit recognition, LeNet-5 may not scale well to larger and\n",
    "              more diverse datasets, where deeper and more complex architectures are often required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a136e83-6c96-4c76-8147-7b886d3929f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Implement LeNet-5 using a deep leacning fcamework of your choice (e.g., TensocFlow, PyTorch) and train\n",
    "   it on a publicly available dataset (e.g., MNIST). Evaluate its pecfocmance and provide\n",
    "   insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb6eff-7922-457d-b5b6-d346bf46aab2",
   "metadata": {},
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa2d6313-f245-465b-9650-ff55a09cb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6b69f-8c9e-455e-8964-c52eed58f34f",
   "metadata": {},
   "source": [
    "## Loading the MNIST Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62f0ab1-2eb3-47dc-b358-0656c72cae97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd665291-dad4-4e3a-b78b-f1a3313774c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ec0de2e-df4a-453b-882a-04b2b6f426b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b6e7f-e076-4009-aabc-34ce5b0776d6",
   "metadata": {},
   "source": [
    "## Normalize pixel values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a57653-e8f5-47e2-8542-3f9643d11603",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2f91b-95c2-4e45-aa0d-f0a86d89af31",
   "metadata": {},
   "source": [
    "## Convert labels to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "258cb326-1311-404a-ac42-0af76fc4d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d452c83-6b21-42d8-803f-38930278523c",
   "metadata": {},
   "source": [
    "## Building the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e63b685-57cf-46a3-96e2-ea7d4d292dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(6, kernel_size = (5,5), padding = 'valid', activation='tanh', input_shape = (28,28,1)))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Conv2D(16, kernel_size = (5,5), padding = 'valid', activation='tanh'))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(300, activation='tanh'))\n",
    "model.add(Dense(100, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "250e36b0-8f2f-4ad3-842a-49aa65211dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 6)         156       \n",
      "                                                                 \n",
      " average_pooling2d_2 (Avera  (None, 12, 12, 6)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 16)          2416      \n",
      "                                                                 \n",
      " average_pooling2d_3 (Avera  (None, 4, 4, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               77100     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110782 (432.74 KB)\n",
      "Trainable params: 110782 (432.74 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae29b38f-0474-41fc-993c-69caf9d49460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0814 - accuracy: 0.9746 - val_loss: 0.0729 - val_accuracy: 0.9766\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0571 - accuracy: 0.9828 - val_loss: 0.0609 - val_accuracy: 0.9809\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0451 - accuracy: 0.9856 - val_loss: 0.0597 - val_accuracy: 0.9825\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0360 - accuracy: 0.9885 - val_loss: 0.0504 - val_accuracy: 0.9837\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0293 - accuracy: 0.9905 - val_loss: 0.0409 - val_accuracy: 0.9857\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0409 - accuracy: 0.9857\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=5, verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1501bbc6-5038-4abf-96d7-170ed1e94c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.040936458855867386\n",
      "Test accuracy: 0.9857000112533569\n"
     ]
    }
   ],
   "source": [
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c969023-fa7f-4ca1-b184-d524a66e1b8c",
   "metadata": {},
   "source": [
    "# 3 - Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c801a-1644-4d49-a8bd-d16730e58344",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Present an overview of the AlexNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6aa72b-50cc-4b3b-bc98-154eab821e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : AlexNet is a deep convolutional neural network architecture that gained significant attention for\n",
    "      its remarkable performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in \n",
    "      2012. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet demonstrated the\n",
    "      effectiveness of deep neural networks for image classification tasks. Here's an overview of the\n",
    "      AlexNet architecture:\n",
    "            \n",
    "    Architecture Overview:\n",
    "        1. Input Layer:\n",
    "               -> AlexNet takes RGB images as input with a fixed size of 224x224 pixels.\n",
    "        2. Convolutional Layers:\n",
    "               -> The network consists of five convolutional layers, with the first layer using a large\n",
    "                  11x11 filter size and the subsequent layers using smaller filter sizes (3x3).\n",
    "               -> The number of filters increases with depth, allowing the network to learn hierarchical\n",
    "                  features.\n",
    "        3.ReLU Activation Function:\n",
    "               -> A Rectified Linear Unit (ReLU) activation function is applied after each convolutional\n",
    "                  layer, introducing non-linearity to the model.\n",
    "        4.Normalization and Pooling Layers:\n",
    "               -> Local Response Normalization (LRN) is applied after the first and second convolutional\n",
    "                  layers to normalize the responses and enhance the model's generalization.\n",
    "               -> Max-pooling layers are used to down-sample the spatial dimensions, providing translation \n",
    "                  invariance.\n",
    "        5.Fully Connected Layers:\n",
    "               -> The convolutional layers are followed by three fully connected layers. The first two fully\n",
    "                  connected layers consist of 4,096 neurons each, and the third fully connected layer has 1,000\n",
    "                  neurons, representing the number of ImageNet classes.\n",
    "               -> Dropout is applied to the fully connected layers during training to prevent overfitting.\n",
    "        6.Softmax Output:\n",
    "                -> The final layer uses a softmax activation function to compute the probability distribution \n",
    "                   over the 1,000 ImageNet classes.\n",
    "                    \n",
    "    Key Innovations:\n",
    "            1.Use of Rectified Linear Units (ReLU):\n",
    "                -> AlexNet was one of the early networks to adopt the ReLU activation function, providing \n",
    "                   faster convergence during training compared to traditional activation functions like sigmoid.\n",
    "            2.Large-Scale Convolutional Filters:\n",
    "                -> The first convolutional layer uses a large 11x11 filter size, capturing a larger receptive \n",
    "                   field and enabling the network to learn more complex features.\n",
    "            3.Data Augmentation and Dropout:\n",
    "                -> Data augmentation techniques, such as image flipping and cropping, were employed to artificially \n",
    "                   increase the size of the training dataset.\n",
    "                -> Dropout was used in the fully connected layers to prevent overfitting.\n",
    "            4.Parallelization with GPUs:\n",
    "                -> AlexNet leveraged the power of Graphics Processing Units (GPUs) for parallelization, allowing \n",
    "                   faster training times and enabling the development of deeper networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c23656-0a5d-4353-b1c9-12a485868f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain the architectural innovations introduced in AlexNet that connected to its breakthrough performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e05bd2-1557-4f64-ae7d-fd86297d31d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : AlexNet introduced several architectural innovations that contributed to its breakthrough performance \n",
    "      in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. These innovations played a \n",
    "      crucial role in demonstrating the effectiveness of deep neural networks for image classification tasks.\n",
    "      Here are the key architectural innovations in AlexNet:\n",
    "    \n",
    "    1. Deep Architecture:\n",
    "        -> AlexNet was one of the first deep convolutional neural networks, featuring multiple layers of \n",
    "           convolutional and fully connected layers. Prior to AlexNet, deep neural networks were less common \n",
    "            due to challenges in training deep models. The depth of AlexNet (eight layers, including five \n",
    "            convolutional layers) allowed it to learn hierarchical features and capture complex patterns in images.\n",
    "    2. ReLU Activation Function:\n",
    "        -> AlexNet replaced traditional activation functions, such as sigmoid or hyperbolic tangent (tanh), \n",
    "           with Rectified Linear Units (ReLU). ReLU introduces non-linearity to the model and accelerates \n",
    "            training by avoiding the vanishing gradient problem associated with earlier activation functions.\n",
    "            This contributed to faster convergence during training.\n",
    "    3. Local Response Normalization (LRN):\n",
    "        -> The first and second convolutional layers in AlexNet utilized Local Response Normalization (LRN).\n",
    "           LRN normalizes the responses within a local neighborhood, enhancing the model's generalization ability. \n",
    "            Although LRN has been somewhat replaced by batch normalization in modern architectures, it played a role\n",
    "            in improving the performance of AlexNet.\n",
    "    4. Large Convolutional Filters:\n",
    "        -> The first convolutional layer of AlexNet used a large 11x11 filter size, capturing a broader receptive \n",
    "           field and allowing the network to learn complex features. Subsequent layers used smaller 3x3 filters. \n",
    "            The use of large filters helped in identifying high-level patterns in the input images.\n",
    "    5. Data Augmentation and Dropout:\n",
    "        -> AlexNet incorporated data augmentation techniques during training, such as image flipping and cropping.\n",
    "           Data augmentation artificially increased the size of the training dataset, improving the model's ability \n",
    "            to generalize to new and unseen data. Additionally, dropout was applied to the fully connected layers,\n",
    "            reducing overfitting by randomly dropping out neurons during training.\n",
    "    6. Parallelization with GPUs:\n",
    "        -> AlexNet was designed to take advantage of Graphics Processing Units (GPUs) for parallelization. \n",
    "           This enabled faster training times, making it practical to train deep neural networks. The \n",
    "            parallelization of computations across GPUs became a common practice in subsequent deep learning architectures.\n",
    "    7.Competition with Ensemble Methods:\n",
    "        -> AlexNet was implemented as an ensemble of multiple models, and the predictions from these models\n",
    "           were combined. This ensemble strategy further improved the model's accuracy and robustness.\n",
    "    8. Top-5 Error Rate Reduction:\n",
    "        -> AlexNet significantly reduced the top-5 error rate in image classification, demonstrating a remarkable \n",
    "           improvement over previous state-of-the-art models. This reduction in error rates highlighted the capability \n",
    "            of deep neural networks to understand and classify complex visual patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2de9d-98ac-4e52-a968-2b18d7e69fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Discuss the role of convolutional layers, pooing layers, and fully connected layers in AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3baf71c-d81d-409a-b479-f34b5ad228bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans : 1. Convolutional Layers:\n",
    "        a.) Role:\n",
    "            -> Convolutional layers are fundamental to the success of AlexNet. They are responsible for\n",
    "               learning local patterns and features in the input images through the application of convolutional filters.\n",
    "            -> The convolutional layers in AlexNet have different filter sizes. The first layer uses a large 11x11 \n",
    "               filter, capturing a large receptive field, while subsequent layers use smaller 3x3 filters. This \n",
    "                allows the network to capture both low-level and high-level features.\n",
    "        b.) Features:\n",
    "            -> Hierarchical Feature Extraction: Convolutional layers capture hierarchical features, starting from \n",
    "               simple edges and textures in the early layers to more complex and abstract representations in the deeper layers.\n",
    "            -> Local Patterns: Filters in convolutional layers act as local pattern detectors, helping the network \n",
    "               recognize features such as edges, corners, and textures.\n",
    "        c.) Activation Function:\n",
    "            -> ReLU activation functions are applied after each convolutional layer, introducing non-linearity to the \n",
    "               model and enabling the network to learn complex mappings.\n",
    "    2. Pooling Layers:\n",
    "        a.) Role:\n",
    "            -> Pooling layers, specifically max-pooling in AlexNet, are used for down-sampling the spatial dimensions\n",
    "               of the feature maps. This reduces the computational complexity of the model and introduces translation invariance.\n",
    "            -> Max-pooling retains the most activated features in a local region, further emphasizing important patterns\n",
    "              while reducing spatial resolution.\n",
    "        b.) Features:\n",
    "            -> Down-Sampling: Pooling layers reduce the spatial dimensions of the feature maps, helping the network focus\n",
    "               on the most salient information and discarding less relevant details.\n",
    "            -> Translation Invariance: Max-pooling contributes to translation invariance, making the model more robust\n",
    "               to slight variations in the position of features within the input images.\n",
    "                \n",
    "    3. Fully Connected Layers:\n",
    "        a.) Role:\n",
    "            -> Fully connected layers follow the convolutional and pooling layers in AlexNet. They integrate\n",
    "               high-level features learned by the previous layers and make decisions based on the extracted representations.\n",
    "            -> The fully connected layers in AlexNet are followed by a softmax layer for multi-class classification.\n",
    "        b.) Features:\n",
    "            -> Decision Making: Fully connected layers are responsible for making decisions about the presence of \n",
    "               specific patterns or objects in the input images. The neurons in these layers are connected to all \n",
    "                neurons in the preceding layer.\n",
    "            -> Global Features: Fully connected layers capture global features and relationships across the entire\n",
    "               input, providing a holistic view of the information learned by the network.\n",
    "        c.) Dropout:\n",
    "            -> Dropout is applied to the fully connected layers during training to prevent overfitting. It \n",
    "               randomly drops out a fraction of neurons, forcing the network to learn more robust and generalized \n",
    "               representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c67db56-238b-409c-886b-41d6d9486b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Implement AlexNet using a deep leacning framework of your choice and evaluate its pecfocmance\n",
    "    on a dataset of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5685c5-6928-4b8a-b866-68b1c2e00273",
   "metadata": {},
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ef7c0e4-6026-40d6-a1cc-ce587a636eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.35.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.25.2-py2.py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.2/184.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.25.2 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.60.0 keras-2.15.0 libclang-16.0.6 markdown-3.5.1 ml-dtypes-0.2.0 opt-einsum-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.35.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb7051b-e2c0-4681-a139-f5d1481952c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 15:12:36.126724: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-28 15:12:36.191723: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-28 15:12:36.191795: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-28 15:12:36.193356: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-28 15:12:36.202592: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-28 15:12:36.203237: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-28 15:12:37.411887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout, Flatten,Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edfaef1c-6c67-42f9-8f42-528f1a7292c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tflearn\n",
      "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tflearn) (1.23.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tflearn) (1.16.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from tflearn) (9.2.0)\n",
      "Building wheels for collected packages: tflearn\n",
      "  Building wheel for tflearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127283 sha256=3f8c0278650a8409a2860b12930f48f1c7c39ca28528247a2db8ea3452238831\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/5d/83/f7/63e33ac9c0560f1dddb2ecff627b8ab6cb076d4b1996416be1\n",
      "Successfully built tflearn\n",
      "Installing collected packages: tflearn\n",
      "Successfully installed tflearn-0.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba70d76-d038-487e-bf38-04d984f7640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6240bf54-a4d0-42c4-a0a4-986bc8df811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54071a54-33cd-4413-be79-186cd6f1c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3018adc5-62c3-45ff-9fda-8bbbcc548d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e25373-df56-4916-a143-b3f77402d7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 5, 5, 96)          11712     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 5, 5, 96)          0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 2, 2, 96)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 2, 2, 96)          384       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 2, 2, 256)         614656    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 1, 1, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 1, 1, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 1, 384)         393600    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 1, 1, 384)         0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 1, 1, 384)         1536      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 1, 384)         1327488   \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1, 1, 384)         0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 1, 1, 384)         1536      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 1, 1, 256)         393472    \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 1, 1, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 1, 1, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 1, 1, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              1052672   \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 4096)              16384     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 4096)              16384     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20654154 (78.79 MB)\n",
      "Trainable params: 20635018 (78.72 MB)\n",
      "Non-trainable params: 19136 (74.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, input_shape=(28,28,1), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation before passing it to the next layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "# 3rd Convolutional Laye\n",
    "model.add(Conv2D(filters=384, kernel_size=(2,2), strides=1, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(2,2), strides=(1,1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "# Passing it to a dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# 1st Dense Layer\n",
    "model.add(Dense(4096, input_shape=(28*28*1,)))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87d31f18-c3b3-4bcd-9931-1cbbfc878d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61810b4-f516-441a-95f2-e93527ce896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "750/750 [==============================] - 101s 128ms/step - loss: 0.3577 - accuracy: 0.9128 - val_loss: 0.1384 - val_accuracy: 0.9629\n",
      "Epoch 2/5\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 0.1703 - accuracy: 0.9556 - val_loss: 0.1153 - val_accuracy: 0.9699\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 0.1180 - accuracy: 0.9686 - val_loss: 0.3383 - val_accuracy: 0.9319\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 93s 124ms/step - loss: 0.1262 - accuracy: 0.9691 - val_loss: 0.0634 - val_accuracy: 0.9837\n",
      "Epoch 5/5\n",
      "477/750 [==================>...........] - ETA: 32s - loss: 0.0930 - accuracy: 0.9759"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=5, verbose=1,validation_split=0.2, shuffle=True)\n",
    "score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77accd9f-c153-4599-83f0-da5aabfcfa67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c4873-459c-4f3b-a230-a53ed1598617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
